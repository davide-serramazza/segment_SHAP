{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:15:35.368184Z",
     "start_time": "2024-08-23T10:15:34.342597Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "#import torch.cuda\n",
    "import timeit\n",
    "import pandas as pd\n",
    "#import argparse\n",
    "import itertools\n",
    "from load_data import load_data # segment_SHAP\n",
    "from evaluation.metrics.AUC_difference import AUIDC_metric\n",
    "from models.predictor_utils import load_predictor\n",
    "from pickle import dump\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:15:35.373483Z",
     "start_time": "2024-08-23T10:15:35.371673Z"
    }
   },
   "outputs": [],
   "source": [
    "# PATHS\n",
    "datasets_folder_path = None\n",
    "attributions_folder_path = None\n",
    "trained_models_folder_path = None\n",
    "save_results_folder_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:15:43.829274Z",
     "start_time": "2024-08-23T10:15:43.826547Z"
    }
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "dataset_names = ['UWAVE']    #[sys.argv[1]] # ['UWAVE', \"KeplerLightCurves\", \"MP8\", \"gunpoint\", \"EOG\"] \n",
    "predictor_names = ['resNet']    #[sys.argv[2]] [\"randomForest\", 'miniRocket', 'resNet', \"QUANT\"]\n",
    "segmentation_names = [\"clasp\",\"greedygaussian\", \"equal\", \"infogain\",\"nnsegment\"] # [\"clasp\",\"greedygaussian\", \"equal\", \"infogain\",\"nnsegment\"] \n",
    "background_names =  [\"average\", \"zero\",\"sampling\"] #[\"average\", \"zero\", \"sampling\"]\n",
    "normalization_names = [\"default\", \"normalized\"]\n",
    "\n",
    "metric_names = [\"AUC_difference\"]\n",
    "\n",
    "demo_mode = True\n",
    "demo_mode_samples = 10\n",
    "# demo\n",
    "if demo_mode:\n",
    "    dataset_names = [\"KeplerLightCurves\"]\n",
    "    predictor_names = [\"randomForest\", 'miniRocket', 'resNet', \"QUANT\"]\n",
    "    segmentation_names = [\"clasp\",\"greedygaussian\", \"equal\", \"infogain\",\"nnsegment\"]\n",
    "    background_names = [\"average\", \"zero\",\"sampling\"] #, 'zero','sampling']\n",
    "    normalization_names = [\"default\", \"normalized\"]\n",
    "\n",
    "# optional\n",
    "# # get infos about which explanations are evaluated\n",
    "# datasets = list( explanations['attributions'].keys() )\n",
    "# segmentations = list( explanations['attributions'][datasets[0]].keys() )\n",
    "# predictors = list( explanations['attributions'][datasets[0]][segmentations[0]].keys() )\n",
    "# backgrounds = list( explanations['attributions'][datasets[0]][segmentations[0]][predictors[0]].keys() )\n",
    "# result_types = ['default','normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:15:44.413374Z",
     "start_time": "2024-08-23T10:15:44.410522Z"
    }
   },
   "outputs": [],
   "source": [
    "# column_names = ['Dataset', 'Segmentation', 'ML model', 'Background', 'Normalization', 'Metric', 'Perturb', \"Result\"]\n",
    "# df = pd.DataFrame(data=data_list, columns = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:15:44.579741Z",
     "start_time": "2024-08-23T10:15:44.576671Z"
    }
   },
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:15:47.048165Z",
     "start_time": "2024-08-23T10:15:45.314377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  UWAVE\n",
      "Predictor:  resNet\n",
      "assessing ('nnsegment', 'average', 'default', 'AUC_difference')\n",
      "elapsed time 9.232853599998634\n",
      "assessing ('nnsegment', 'average', 'normalized', 'AUC_difference')\n",
      "elapsed time 17.830368999988423\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "if datasets_folder_path is None:\n",
    "    datasets_folder_path = \"datasets\" #os.path.join(cwd, \"datasets\")\n",
    "if attributions_folder_path is None:\n",
    "    attributions_folder_path = \"attributions\"\n",
    "if trained_models_folder_path is None:\n",
    "    trained_models_folder_path = \"trained_models\"\n",
    "if save_results_folder_path is None:\n",
    "    save_results_folder_path = os.path.join(\"evaluation\", \"results\")\n",
    "\n",
    "# device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "starttime = timeit.default_timer()\n",
    "\n",
    "eval_metrics = dict.fromkeys(metric_names)\n",
    "for key in eval_metrics:\n",
    "    if key==\"AUC_difference\":\n",
    "        eval_metrics[key] = AUIDC_metric()\n",
    "    else:\n",
    "        raise KeyError(f\"key {key} has no corresponding eval metric defined\")\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(\"Dataset: \", dataset_name)\n",
    "    # loading dataset\n",
    "    X_train, X_test, y_train, y_test, enc = load_data(subset='all', dataset_name=dataset_name, path=datasets_folder_path)\n",
    "    if demo_mode:\n",
    "        n_test_samples = X_test.shape[0]\n",
    "        n_samples_to_choose = np.min([n_test_samples, demo_mode_samples])\n",
    "        demo_mode_idxs = np.random.choice(n_test_samples, demo_mode_samples, replace=False)\n",
    "\n",
    "        X_test, y_test = X_test[demo_mode_idxs], y_test[demo_mode_idxs]\n",
    "\n",
    "    for eval_metric in eval_metrics.values():\n",
    "        eval_metric.fit_data(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    for predictor_name in predictor_names:\n",
    "        print(\"Predictor: \", predictor_name)\n",
    "\n",
    "        # load classifier\n",
    "        predictor = load_predictor(path=trained_models_folder_path, predictor_name=predictor_name, dataset_name=dataset_name, device=device) # torch.device(device)\n",
    "\n",
    "        # load explanations\n",
    "        attribution_filename = \"_\".join((\"all_results\", dataset_name, predictor_name)) + \".npy\"\n",
    "        explanations = np.load(os.path.join(attributions_folder_path, attribution_filename), allow_pickle=True).item() # FileNotFoundError\n",
    "        label_mapping = explanations['label_mapping'][dataset_name]\n",
    "\n",
    "        for eval_metric in eval_metrics.values():\n",
    "            eval_metric.fit_ml_model(predictor)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "        for key in itertools.product(segmentation_names, background_names, normalization_names, metric_names):\n",
    "            segmentation_name, background_name, normalization_name, metric_name = key\n",
    "            print(\"assessing\", key)\n",
    "\n",
    "            # load model and explanations to access\n",
    "            try:\n",
    "                attributions = explanations['attributions'][dataset_name][segmentation_name][predictor_name][background_name][normalization_name]\n",
    "            except KeyError as error:\n",
    "                print('Warning: attributions is missing keys, skipping to next ' + repr(error))\n",
    "                continue\n",
    "            try:\n",
    "                y_test_pred = explanations['y_test_pred'][dataset_name][predictor_name]\n",
    "            except KeyError:\n",
    "                y_test_pred = explanations['y_test_pred'][dataset_name][segmentation_name][predictor_name]\n",
    "\n",
    "            eval_metric = eval_metrics[metric_name]\n",
    "            results = eval_metric.evaluate(attributions, y_test_pred, predictor)\n",
    "\n",
    "            for result_tuple in results:\n",
    "                data_list.append((dataset_name, segmentation_name, predictor_name, background_name, normalization_name, metric_name) + result_tuple)\n",
    "\n",
    "            print(\"elapsed time\", (timeit.default_timer() - starttime))\n",
    "\n",
    "        # save\n",
    "        column_names = ['Dataset', 'Segmentation', 'ML model', 'Background', 'Normalization', 'Metric', 'Perturb', \"Result\"]\n",
    "        df = pd.DataFrame(data=data_list, columns = column_names)\n",
    "        file_name = \"_\".join((\"evaluation\", predictor_name, dataset_name))\n",
    "        result_path = os.path.join(save_results_folder_path, file_name)\n",
    "        if not demo_mode:\n",
    "            df.to_csv(result_path)\n",
    "            # with open( \"_\".join( (dataset_name,classifier_name)) ,\"wb\") as f:\n",
    "            #     pickle.dump(results_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Segmentation</th>\n",
       "      <th>ML model</th>\n",
       "      <th>Background</th>\n",
       "      <th>Normalization</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Perturb</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UWAVE</td>\n",
       "      <td>nnsegment</td>\n",
       "      <td>resNet</td>\n",
       "      <td>average</td>\n",
       "      <td>default</td>\n",
       "      <td>AUC_difference</td>\n",
       "      <td>default</td>\n",
       "      <td>0.342571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UWAVE</td>\n",
       "      <td>nnsegment</td>\n",
       "      <td>resNet</td>\n",
       "      <td>average</td>\n",
       "      <td>normalized</td>\n",
       "      <td>AUC_difference</td>\n",
       "      <td>default</td>\n",
       "      <td>0.347857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset Segmentation ML model Background Normalization          Metric  \\\n",
       "0   UWAVE    nnsegment   resNet    average       default  AUC_difference   \n",
       "1   UWAVE    nnsegment   resNet    average    normalized  AUC_difference   \n",
       "\n",
       "   Perturb    Result  \n",
       "0  default  0.342571  \n",
       "1  default  0.347857  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segment_shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
