{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "#import torch.cuda\n",
    "import timeit\n",
    "import pandas as pd\n",
    "#import argparse\n",
    "import itertools\n",
    "from load_data import load_data # segment_SHAP\n",
    "from evaluation.metrics.AUC_difference import AUC_difference\n",
    "from predictor_utils import load_predictor, predict_proba\n",
    "from pickle import dump\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "datasets_folder_path = None\n",
    "attributions_folder_path = None\n",
    "trained_models_folder_path = None\n",
    "save_results_folder_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "dataset_names = ['MP']    #[sys.argv[1]]\n",
    "predictor_names = ['randomForest']    #[sys.argv[2]] [\"randomForest\", 'miniRocket', 'resNet', \"QUANT\"]\n",
    "segmentation_names = [\"clasp\",\"greedygaussian\", \"equal\", \"infogain\",\"nnsegment\"] # [\"clasp\",\"greedygaussian\", \"equal\", \"infogain\",\"nnsegment\"] \n",
    "background_names =  [\"average\", \"zero\",\"sampling\"] #[\"average\", \"zero\", \"sampling\"]\n",
    "normalization_names = [\"default\", \"normalized\"]\n",
    "\n",
    "metric_names = [\"AUC_difference\"]\n",
    "\n",
    "demo_mode = True\n",
    "# demo\n",
    "if demo_mode:\n",
    "    dataset_names = [\"UWAVE\"] # 'gunpoint'\n",
    "    predictor_names = ['resNet', 'randomForest'] #['randomForest', 'resNet', 'miniRocket']\n",
    "    segmentation_names = ['clasp'] # [\"clasp\",\"greedygaussian\", \"equal\", \"infogain\",\"nnsegment\"] \n",
    "    background_names = [\"average\", 'zero'] # [\"average\", \"zero\", \"sampling\"]\n",
    "    normalization_names = [\"default\", \"normalized\"]\n",
    "\n",
    "# optional\n",
    "# # get infos about which explanations are evaluated\n",
    "# datasets = list( explanations['attributions'].keys() )\n",
    "# segmentations = list( explanations['attributions'][datasets[0]].keys() )\n",
    "# predictors = list( explanations['attributions'][datasets[0]][segmentations[0]].keys() )\n",
    "# backgrounds = list( explanations['attributions'][datasets[0]][segmentations[0]][predictors[0]].keys() )\n",
    "# result_types = ['default','normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  gunpoint\n",
      "Predictor:  resNet\n",
      "assessing ('clasp', 'average', 'default', 'AUC_difference')\n",
      "[ 3.9550397 -5.7800856]\n",
      "[-3.5554402  2.6495543]\n",
      "elapsed time 0.6187920999946073\n",
      "assessing ('clasp', 'average', 'normalized', 'AUC_difference')\n",
      "[ 3.9550397 -5.7800856]\n",
      "[-3.5554402  2.6495543]\n",
      "elapsed time 1.1730230000102893\n",
      "assessing ('clasp', 'zero', 'default', 'AUC_difference')\n",
      "[ 3.9550397 -5.7800856]\n",
      "[-3.5554402  2.6495543]\n",
      "elapsed time 1.6585206999443471\n",
      "assessing ('clasp', 'zero', 'normalized', 'AUC_difference')\n",
      "[ 3.9550397 -5.7800856]\n",
      "[-3.5554402  2.6495543]\n",
      "elapsed time 2.129864600021392\n",
      "Predictor:  randomForest\n",
      "assessing ('clasp', 'average', 'default', 'AUC_difference')\n",
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m     y_test_pred \u001b[38;5;241m=\u001b[39m explanations[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_test_pred\u001b[39m\u001b[38;5;124m'\u001b[39m][dataset_name][segmentation_name][predictor_name]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metric_name\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUC_difference\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 53\u001b[0m     results \u001b[38;5;241m=\u001b[39m AUC_difference(predictor, X_train, X_test, y_train, attributions, y_test_pred, label_mapping, n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m18\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Nikos\\OneDrive - University of Bristol\\Projects\\segmentshap project\\segment_SHAP\\evaluation\\metrics\\AUC_difference.py:72\u001b[0m, in \u001b[0;36mAUC_difference\u001b[1;34m(classifier, X_train, X_test, y_train, attributions, y_test_pred, label_mapping, n_steps)\u001b[0m\n\u001b[0;32m     70\u001b[0m sample_class_pred_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(sample_pred)\n\u001b[0;32m     71\u001b[0m sample_class \u001b[38;5;241m=\u001b[39m classes[sample_class_pred_idx]\n\u001b[1;32m---> 72\u001b[0m sample_class_pred \u001b[38;5;241m=\u001b[39m sample_pred[sample_class_pred_idx]\n\u001b[0;32m     73\u001b[0m opposite_sample \u001b[38;5;241m=\u001b[39m opposite_class_dict[sample_class][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     74\u001b[0m opposite_class_pred \u001b[38;5;241m=\u001b[39m opposite_class_dict[sample_class][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m\"\u001b[39m][sample_class_pred_idx]\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "if datasets_folder_path is None:\n",
    "    datasets_folder_path = \"datasets\" #os.path.join(cwd, \"datasets\")\n",
    "if attributions_folder_path is None:\n",
    "    attributions_folder_path = \"attributions\"\n",
    "if trained_models_folder_path is None:\n",
    "    trained_models_folder_path = \"trained_models\"\n",
    "if save_results_folder_path is None:\n",
    "    save_results_folder_path = os.path.join(\"evaluation\", \"results\")\n",
    "\n",
    "# device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "starttime = timeit.default_timer()\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(\"Dataset: \", dataset_name)\n",
    "    # loading dataset\n",
    "    X_train, X_test, y_train, y_test, enc = load_data(subset='all', dataset_name=dataset_name, path=datasets_folder_path)\n",
    "    if demo_mode:\n",
    "        X_test, y_test = X_test[:2], y_test[:2]\n",
    "\n",
    "\n",
    "    for predictor_name in predictor_names:\n",
    "        print(\"Predictor: \", predictor_name)\n",
    "\n",
    "        # load classifier\n",
    "        predictor = load_predictor(path=trained_models_folder_path, predictor_name=predictor_name, dataset_name=dataset_name, device=device) # torch.device(device)\n",
    "\n",
    "        # load explanations\n",
    "        attribution_filename = \"_\".join((\"all_results\", dataset_name, predictor_name)) + \".npy\"\n",
    "        explanations = np.load(os.path.join(attributions_folder_path, attribution_filename), allow_pickle=True).item() # FileNotFoundError\n",
    "        label_mapping = explanations['label_mapping'][dataset_name]\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "        for key in itertools.product(segmentation_names, background_names, normalization_names, metric_names):\n",
    "            segmentation_name, background_name, normalization_name, metric_name = key\n",
    "            print(\"assessing\", key)\n",
    "\n",
    "            # load model and explanations to access\n",
    "            try:\n",
    "                attributions = explanations['attributions'][dataset_name][segmentation_name][predictor_name][background_name][normalization_name]\n",
    "            except KeyError as error:\n",
    "                print('Warning: attributions is missing keys, skipping to next ' + repr(error))\n",
    "                continue\n",
    "            try:\n",
    "                y_test_pred = explanations['y_test_pred'][dataset_name][predictor_name]\n",
    "            except KeyError:\n",
    "                y_test_pred = explanations['y_test_pred'][dataset_name][segmentation_name][predictor_name]\n",
    "\n",
    "            if metric_name==\"AUC_difference\":\n",
    "                results = AUC_difference(predictor, X_train, X_test, y_train, attributions, y_test_pred, label_mapping, n_steps=18)\n",
    "            else:\n",
    "                raise ValueError(f\"metric name {metric_name} is not supported\")\n",
    "            for result_tuple in results:\n",
    "                data_list.append((dataset_name, segmentation_name, predictor_name, background_name, normalization_name, metric_name) + result_tuple)\n",
    "\n",
    "            print(\"elapsed time\", (timeit.default_timer() - starttime))\n",
    "\n",
    "        # save\n",
    "        column_names = ['Dataset', 'Segmentation', 'ML model', 'Background', 'Normalization', 'Metric', 'Perturb', \"Result\"]\n",
    "        df = pd.DataFrame(data=data_list, columns = column_names)\n",
    "        file_name = \"_\".join((\"evaluation\", predictor_name, dataset_name))\n",
    "        result_path = os.path.join(save_results_folder_path, file_name)\n",
    "        if not demo_mode:\n",
    "            df.to_csv(result_path)\n",
    "            # with open( \"_\".join( (dataset_name,classifier_name)) ,\"wb\") as f:\n",
    "            #     pickle.dump(results_dict, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names = ['Dataset', 'Segmentation', 'ML model', 'Background', 'Normalization', 'Metric', 'Perturb', \"Result\"]\n",
    "# df = pd.DataFrame(data=data_list, columns = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Segmentation</th>\n",
       "      <th>ML model</th>\n",
       "      <th>Background</th>\n",
       "      <th>Normalization</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Perturb</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UWAVE</td>\n",
       "      <td>clasp</td>\n",
       "      <td>randomForest</td>\n",
       "      <td>average</td>\n",
       "      <td>default</td>\n",
       "      <td>AUC_difference</td>\n",
       "      <td>default</td>\n",
       "      <td>0.300526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UWAVE</td>\n",
       "      <td>clasp</td>\n",
       "      <td>randomForest</td>\n",
       "      <td>average</td>\n",
       "      <td>default</td>\n",
       "      <td>AUC_difference</td>\n",
       "      <td>normalized</td>\n",
       "      <td>0.502729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UWAVE</td>\n",
       "      <td>clasp</td>\n",
       "      <td>randomForest</td>\n",
       "      <td>average</td>\n",
       "      <td>normalized</td>\n",
       "      <td>AUC_difference</td>\n",
       "      <td>default</td>\n",
       "      <td>0.350263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UWAVE</td>\n",
       "      <td>clasp</td>\n",
       "      <td>randomForest</td>\n",
       "      <td>average</td>\n",
       "      <td>normalized</td>\n",
       "      <td>AUC_difference</td>\n",
       "      <td>normalized</td>\n",
       "      <td>0.644951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UWAVE</td>\n",
       "      <td>clasp</td>\n",
       "      <td>randomForest</td>\n",
       "      <td>zero</td>\n",
       "      <td>default</td>\n",
       "      <td>AUC_difference</td>\n",
       "      <td>default</td>\n",
       "      <td>0.308947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UWAVE</td>\n",
       "      <td>clasp</td>\n",
       "      <td>randomForest</td>\n",
       "      <td>zero</td>\n",
       "      <td>default</td>\n",
       "      <td>AUC_difference</td>\n",
       "      <td>normalized</td>\n",
       "      <td>0.529440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UWAVE</td>\n",
       "      <td>clasp</td>\n",
       "      <td>randomForest</td>\n",
       "      <td>zero</td>\n",
       "      <td>normalized</td>\n",
       "      <td>AUC_difference</td>\n",
       "      <td>default</td>\n",
       "      <td>0.372632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UWAVE</td>\n",
       "      <td>clasp</td>\n",
       "      <td>randomForest</td>\n",
       "      <td>zero</td>\n",
       "      <td>normalized</td>\n",
       "      <td>AUC_difference</td>\n",
       "      <td>normalized</td>\n",
       "      <td>0.755038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset Segmentation      ML model Background Normalization          Metric  \\\n",
       "0   UWAVE        clasp  randomForest    average       default  AUC_difference   \n",
       "1   UWAVE        clasp  randomForest    average       default  AUC_difference   \n",
       "2   UWAVE        clasp  randomForest    average    normalized  AUC_difference   \n",
       "3   UWAVE        clasp  randomForest    average    normalized  AUC_difference   \n",
       "4   UWAVE        clasp  randomForest       zero       default  AUC_difference   \n",
       "5   UWAVE        clasp  randomForest       zero       default  AUC_difference   \n",
       "6   UWAVE        clasp  randomForest       zero    normalized  AUC_difference   \n",
       "7   UWAVE        clasp  randomForest       zero    normalized  AUC_difference   \n",
       "\n",
       "      Perturb    Result  \n",
       "0     default  0.300526  \n",
       "1  normalized  0.502729  \n",
       "2     default  0.350263  \n",
       "3  normalized  0.644951  \n",
       "4     default  0.308947  \n",
       "5  normalized  0.529440  \n",
       "6     default  0.372632  \n",
       "7  normalized  0.755038  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(428, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_segment_shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
