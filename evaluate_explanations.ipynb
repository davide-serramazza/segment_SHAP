{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:15:35.368184Z",
     "start_time": "2024-08-23T10:15:34.342597Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "#sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "#import torch.cuda\n",
    "import timeit\n",
    "import pandas as pd\n",
    "#import argparse\n",
    "import itertools\n",
    "from load_data import load_data # segment_SHAP\n",
    "from evaluation.metrics.AUC_difference import AUC_difference\n",
    "from models.predictor_utils import load_predictor\n",
    "from pickle import dump\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:15:35.373483Z",
     "start_time": "2024-08-23T10:15:35.371673Z"
    }
   },
   "source": [
    "# PATHS\n",
    "datasets_folder_path = None\n",
    "attributions_folder_path = None\n",
    "trained_models_folder_path = None\n",
    "save_results_folder_path = None"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:15:43.829274Z",
     "start_time": "2024-08-23T10:15:43.826547Z"
    }
   },
   "source": [
    "# settings\n",
    "dataset_names = ['gunpoint']    #[sys.argv[1]]\n",
    "predictor_names = ['resNet']    #[sys.argv[2]] [\"randomForest\", 'miniRocket', 'resNet', \"QUANT\"]\n",
    "segmentation_names = [\"clasp\",\"greedygaussian\", \"equal\", \"infogain\",\"nnsegment\"] # [\"clasp\",\"greedygaussian\", \"equal\", \"infogain\",\"nnsegment\"] \n",
    "background_names =  [\"average\", \"zero\",\"sampling\"] #[\"average\", \"zero\", \"sampling\"]\n",
    "normalization_names = [\"default\", \"normalized\"]\n",
    "\n",
    "metric_names = [\"AUC_difference\"]\n",
    "\n",
    "demo_mode = True\n",
    "# demo\n",
    "if demo_mode:\n",
    "    dataset_names = ['gunpoint']\n",
    "    predictor_names = ['miniRocket'] #['randomForest', 'resNet', 'miniRocket']\n",
    "    segmentation_names = ['equal']\n",
    "    background_names = [\"average\"]#, 'zero','sampling']\n",
    "    normalization_names = [\"default\", \"normalized\"]\n",
    "\n",
    "# optional\n",
    "# # get infos about which explanations are evaluated\n",
    "# datasets = list( explanations['attributions'].keys() )\n",
    "# segmentations = list( explanations['attributions'][datasets[0]].keys() )\n",
    "# predictors = list( explanations['attributions'][datasets[0]][segmentations[0]].keys() )\n",
    "# backgrounds = list( explanations['attributions'][datasets[0]][segmentations[0]][predictors[0]].keys() )\n",
    "# result_types = ['default','normalized']"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:15:44.413374Z",
     "start_time": "2024-08-23T10:15:44.410522Z"
    }
   },
   "source": [
    "# column_names = ['Dataset', 'Segmentation', 'ML model', 'Background', 'Normalization', 'Metric', 'Perturb', \"Result\"]\n",
    "# df = pd.DataFrame(data=data_list, columns = column_names)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:15:44.579741Z",
     "start_time": "2024-08-23T10:15:44.576671Z"
    }
   },
   "source": [
    "# df"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:15:44.849754Z",
     "start_time": "2024-08-23T10:15:44.846952Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:15:44.976831Z",
     "start_time": "2024-08-23T10:15:44.974960Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:15:47.048165Z",
     "start_time": "2024-08-23T10:15:45.314377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cwd = os.getcwd()\n",
    "if datasets_folder_path is None:\n",
    "    datasets_folder_path = \"datasets\" #os.path.join(cwd, \"datasets\")\n",
    "if attributions_folder_path is None:\n",
    "    attributions_folder_path = \"attributions\"\n",
    "if trained_models_folder_path is None:\n",
    "    trained_models_folder_path = \"models/trained_models\"\n",
    "if save_results_folder_path is None:\n",
    "    save_results_folder_path = os.path.join(\"evaluation\", \"results\")\n",
    "\n",
    "# device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "starttime = timeit.default_timer()\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(\"Dataset: \", dataset_name)\n",
    "    # loading dataset\n",
    "    X_train, X_test, y_train, y_test, enc = load_data(subset='all', dataset_name=dataset_name, path=datasets_folder_path)\n",
    "    if demo_mode:\n",
    "        X_test, y_test = X_test[:2], y_test[:2]\n",
    "\n",
    "\n",
    "    for predictor_name in predictor_names:\n",
    "        print(\"Predictor: \", predictor_name)\n",
    "\n",
    "        # load classifier\n",
    "        predictor = load_predictor(path=trained_models_folder_path, predictor_name=predictor_name, dataset_name=dataset_name, device=device) # torch.device(device)\n",
    "\n",
    "        # load explanations\n",
    "        attribution_filename = \"_\".join((\"all_results\", dataset_name, predictor_name)) + \".npy\"\n",
    "        explanations = np.load(os.path.join(attributions_folder_path, attribution_filename), allow_pickle=True).item() # FileNotFoundError\n",
    "        label_mapping = explanations['label_mapping'][dataset_name]\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "        for key in itertools.product(segmentation_names, background_names, normalization_names, metric_names):\n",
    "            segmentation_name, background_name, normalization_name, metric_name = key\n",
    "            print(\"assessing\", key)\n",
    "\n",
    "            # load model and explanations to access\n",
    "            try:\n",
    "                attributions = explanations['attributions'][dataset_name][segmentation_name][predictor_name][background_name][normalization_name]\n",
    "            except KeyError as error:\n",
    "                print('Warning: attributions is missing keys, skipping to next ' + repr(error))\n",
    "                continue\n",
    "            try:\n",
    "                y_test_pred = explanations['y_test_pred'][dataset_name][predictor_name]\n",
    "            except KeyError:\n",
    "                y_test_pred = explanations['y_test_pred'][dataset_name][segmentation_name][predictor_name]\n",
    "\n",
    "            if metric_name==\"AUC_difference\":\n",
    "                results = AUC_difference(predictor, X_train, X_test, y_train, attributions, y_test_pred, label_mapping, n_steps=18)\n",
    "            else:\n",
    "                raise ValueError(f\"metric name {metric_name} is not supported\")\n",
    "            for result_tuple in results:\n",
    "                data_list.append((dataset_name, segmentation_name, predictor_name, background_name, normalization_name, metric_name) + result_tuple)\n",
    "\n",
    "            print(\"elapsed time\", (timeit.default_timer() - starttime))\n",
    "\n",
    "        # save\n",
    "        column_names = ['Dataset', 'Segmentation', 'ML model', 'Background', 'Normalization', 'Metric', 'Perturb', \"Result\"]\n",
    "        df = pd.DataFrame(data=data_list, columns = column_names)\n",
    "        file_name = \"_\".join((\"evaluation\", predictor_name, dataset_name))\n",
    "        result_path = os.path.join(save_results_folder_path, file_name)\n",
    "        if not demo_mode:\n",
    "            df.to_csv(result_path)\n",
    "            # with open( \"_\".join( (dataset_name,classifier_name)) ,\"wb\") as f:\n",
    "            #     pickle.dump(results_dict, f)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  gunpoint\n",
      "Predictor:  miniRocket\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported predictor extension: models/trained_models",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 28\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPredictor: \u001B[39m\u001B[38;5;124m\"\u001B[39m, predictor_name)\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# load classifier\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m predictor \u001B[38;5;241m=\u001B[39m load_predictor(path\u001B[38;5;241m=\u001B[39mtrained_models_folder_path, predictor_name\u001B[38;5;241m=\u001B[39mpredictor_name, dataset_name\u001B[38;5;241m=\u001B[39mdataset_name, device\u001B[38;5;241m=\u001B[39mdevice) \u001B[38;5;66;03m# torch.device(device)\u001B[39;00m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# load explanations\u001B[39;00m\n\u001B[1;32m     31\u001B[0m attribution_filename \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin((\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall_results\u001B[39m\u001B[38;5;124m\"\u001B[39m, dataset_name, predictor_name)) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.npy\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/workspace/PhD/bristolSHAP/models/predictor_utils.py:37\u001B[0m, in \u001B[0;36mload_predictor\u001B[0;34m(path, predictor_name, dataset_name, device)\u001B[0m\n\u001B[1;32m     35\u001B[0m \tpredictor \u001B[38;5;241m=\u001B[39m Sequential(predictor, Softmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 37\u001B[0m \t\u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUnsupported predictor extension: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m predictor\n",
      "\u001B[0;31mValueError\u001B[0m: Unsupported predictor extension: models/trained_models"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_segment_shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
