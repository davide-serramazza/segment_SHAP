{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "#import torch.cuda\n",
    "import timeit\n",
    "import pandas as pd\n",
    "#import argparse\n",
    "import itertools\n",
    "from load_data import load_data # segment_SHAP\n",
    "from evaluation.metrics.AUC_difference import AUC_difference\n",
    "from predictor_utils import load_predictor, predict_proba\n",
    "from pickle import dump\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "datasets_folder_path = None\n",
    "attributions_folder_path = None\n",
    "trained_models_folder_path = None\n",
    "save_results_folder_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "dataset_names = ['MP']    #[sys.argv[1]]\n",
    "predictor_names = ['randomForest']    #[sys.argv[2]] [\"randomForest\", 'miniRocket', 'resNet', \"QUANT\"]\n",
    "segmentation_names = [\"clasp\",\"greedygaussian\", \"equal\", \"infogain\",\"nnsegment\"] # [\"clasp\",\"greedygaussian\", \"equal\", \"infogain\",\"nnsegment\"] \n",
    "background_names =  [\"average\", \"zero\",\"sampling\"] #[\"average\", \"zero\", \"sampling\"]\n",
    "normalization_names = [\"default\", \"normalized\"]\n",
    "\n",
    "metric_names = [\"AUC_difference\"]\n",
    "\n",
    "demo_mode = True\n",
    "# demo\n",
    "if demo_mode:\n",
    "    dataset_names = ['UWAVE']\n",
    "    predictor_names = ['resNet', 'randomForest'] #['randomForest', 'resNet', 'miniRocket']\n",
    "    segmentation_names = ['clasp']\n",
    "    background_names = [\"average\", 'zero'] #,'sampling']\n",
    "    normalization_names = [\"default\", \"normalized\"]\n",
    "\n",
    "# optional\n",
    "# # get infos about which explanations are evaluated\n",
    "# datasets = list( explanations['attributions'].keys() )\n",
    "# segmentations = list( explanations['attributions'][datasets[0]].keys() )\n",
    "# predictors = list( explanations['attributions'][datasets[0]][segmentations[0]].keys() )\n",
    "# backgrounds = list( explanations['attributions'][datasets[0]][segmentations[0]][predictors[0]].keys() )\n",
    "# result_types = ['default','normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  UWAVE\n",
      "Predictor:  resNet\n",
      "assessing ('clasp', 'average', 'default', 'AUC_difference')\n",
      "(428, 8)\n",
      "elapsed time 0.9670077000046149\n",
      "assessing ('clasp', 'average', 'normalized', 'AUC_difference')\n",
      "(428, 8)\n",
      "elapsed time 1.7912940999958664\n",
      "assessing ('clasp', 'zero', 'default', 'AUC_difference')\n",
      "(428, 8)\n",
      "elapsed time 2.568037200020626\n",
      "assessing ('clasp', 'zero', 'normalized', 'AUC_difference')\n",
      "(428, 8)\n",
      "elapsed time 3.3488725000061095\n",
      "Predictor:  randomForest\n",
      "assessing ('clasp', 'average', 'default', 'AUC_difference')\n",
      "(428, 8)\n",
      "elapsed time 4.052089700009674\n",
      "assessing ('clasp', 'average', 'normalized', 'AUC_difference')\n",
      "(428, 8)\n",
      "elapsed time 4.651207800023258\n",
      "assessing ('clasp', 'zero', 'default', 'AUC_difference')\n",
      "(428, 8)\n",
      "elapsed time 5.204801800020505\n",
      "assessing ('clasp', 'zero', 'normalized', 'AUC_difference')\n",
      "(428, 8)\n",
      "elapsed time 5.76889609999489\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "if datasets_folder_path is None:\n",
    "    datasets_folder_path = \"datasets\" #os.path.join(cwd, \"datasets\")\n",
    "if attributions_folder_path is None:\n",
    "    attributions_folder_path = \"attributions\"\n",
    "if trained_models_folder_path is None:\n",
    "    trained_models_folder_path = \"trained_models\"\n",
    "if save_results_folder_path is None:\n",
    "    save_results_folder_path = os.path.join(\"evaluation\", \"results\")\n",
    "\n",
    "# device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "starttime = timeit.default_timer()\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(\"Dataset: \", dataset_name)\n",
    "    # loading dataset\n",
    "    X_train, X_test, y_train, y_test, enc = load_data(subset='all', dataset_name=dataset_name, path=datasets_folder_path)\n",
    "    if demo_mode:\n",
    "        X_test, y_test = X_test[:2], y_test[:2]\n",
    "\n",
    "\n",
    "    for predictor_name in predictor_names:\n",
    "        print(\"Predictor: \", predictor_name)\n",
    "\n",
    "        # load classifier\n",
    "        predictor = load_predictor(path=trained_models_folder_path, predictor_name=predictor_name, dataset_name=dataset_name, device=device) # torch.device(device)\n",
    "\n",
    "        # load explanations\n",
    "        attribution_filename = \"_\".join((\"all_results\", dataset_name, predictor_name)) + \".npy\"\n",
    "        explanations = np.load(os.path.join(attributions_folder_path, attribution_filename), allow_pickle=True).item() # FileNotFoundError\n",
    "        label_mapping = explanations['label_mapping'][dataset_name]\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "        for key in itertools.product(segmentation_names, background_names, normalization_names, metric_names):\n",
    "            segmentation_name, background_name, normalization_name, metric_name = key\n",
    "            print(\"assessing\", key)\n",
    "\n",
    "            # load model and explanations to access\n",
    "            try:\n",
    "                attributions = explanations['attributions'][dataset_name][segmentation_name][predictor_name][background_name][normalization_name]\n",
    "            except KeyError as error:\n",
    "                print('Warning: attributions is missing keys, skipping to next ' + repr(error))\n",
    "                continue\n",
    "            try:\n",
    "                y_test_pred = explanations['y_test_pred'][dataset_name][predictor_name]\n",
    "            except KeyError:\n",
    "                y_test_pred = explanations['y_test_pred'][dataset_name][segmentation_name][predictor_name]\n",
    "\n",
    "            if metric_name==\"AUC_difference\":\n",
    "                results = AUC_difference(predictor, X_train, X_test, y_train, attributions, y_test_pred, label_mapping, n_steps=18)\n",
    "            else:\n",
    "                raise ValueError(f\"metric name {metric_name} is not supported\")\n",
    "            for result_tuple in results:\n",
    "                data_list.append((dataset_name, segmentation_name, predictor_name, background_name, normalization_name, metric_name) + result_tuple)\n",
    "\n",
    "            print(\"elapsed time\", (timeit.default_timer() - starttime))\n",
    "\n",
    "        # save\n",
    "        column_names = ['Dataset', 'Segmentation', 'ML model', 'Background', 'Normalization', 'Metric', 'Perturb', \"Result\"]\n",
    "        df = pd.DataFrame(data=data_list, columns = column_names)\n",
    "        file_name = \"_\".join((\"evaluation\", predictor_name, dataset_name))\n",
    "        result_path = os.path.join(save_results_folder_path, file_name)\n",
    "        if not demo_mode:\n",
    "            df.to_csv(result_path)\n",
    "            # with open( \"_\".join( (dataset_name,classifier_name)) ,\"wb\") as f:\n",
    "            #     pickle.dump(results_dict, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names = ['Dataset', 'Segmentation', 'ML model', 'Background', 'Normalization', 'Metric', 'Perturb', \"Result\"]\n",
    "# df = pd.DataFrame(data=data_list, columns = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(428, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_segment_shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
