{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:21:32.432775Z",
     "start_time": "2024-06-11T15:21:30.579392Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nikos\\miniconda3\\envs\\env_segment_shap\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from captum.attr import ShapleyValueSampling\n",
    "from tqdm import trange\n",
    "\n",
    "from load_data import load_data\n",
    "from train_models import *\n",
    "from segmentation import *\n",
    "from utils import *\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:21:32.606460Z",
     "start_time": "2024-06-11T15:21:32.437520Z"
    }
   },
   "outputs": [],
   "source": [
    "# device for torch\n",
    "from torch.cuda import is_available as is_GPU_available\n",
    "device = \"cuda\" if is_GPU_available() else \"cpu\"\n",
    "\n",
    "# dictionary mapping predictors to torch vs other, step necessary for Captum \n",
    "predictors = {\n",
    "\t'torch' : ['resNet'],\n",
    "\t'scikit' : ['miniRocket','randomForest']\n",
    "}\n",
    "\n",
    "segmentations = {\"clasp\":get_claSP_segmentation, \"infogain\": get_InformationGain_segmentation, \"greedygaussian\": get_GreedyGaussian_segmentation, \"equal\": get_equal_segmentation, \"nnsegment\": get_NNSegment_segmentation}\n",
    "\n",
    "segmentation_types = {\"default\", \"normalized\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:21:34.522464Z",
     "start_time": "2024-06-11T15:21:32.607267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training random forest\n",
      "random forest accuracy is 1.0\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dataset_name = 'gunpoint'\n",
    "X_train, X_test, y_train, y_test, enc = load_data(subset='all', dataset_name=dataset_name)\n",
    "# for debugging only\n",
    "# X_test = X_test[:2]\n",
    "# y_test = y_test[:2]\n",
    "n_samples, n_chs, ts_length = X_test.shape\n",
    "\n",
    "# train model\n",
    "predictor_name = 'randomForest'\n",
    "if predictor_name=='resNet':\n",
    "\tclf,preds = train_ResNet(X_train, y_train, X_test, y_test, dataset_name,device=device)\n",
    "elif predictor_name=='miniRocket':\n",
    "\tclf,preds = train_miniRocket(X_train, y_train, X_test, y_test, dataset_name)\n",
    "elif predictor_name==\"randomForest\":\n",
    "\tclf, preds = train_randomForest(X_train, y_train, X_test, y_test, dataset_name)\n",
    "else:\n",
    "\traise ValueError(\n",
    "\t\t\"predictor not found\"\n",
    "\t)\n",
    "\n",
    "# segmentation\n",
    "segmentation_name = \"nnsegment\"\n",
    "segmentation_type = \"normalized\"\n",
    "segmentation_method = segmentations[segmentation_name]\n",
    "\n",
    "# initialize data structure meant to contain the segments\n",
    "# TODO can I be cleaner here?\n",
    "segments =  np.empty( (X_test.shape[0] , X_test.shape[1]), dtype=object) if X_test.shape[1] > 1  else (\n",
    "\tnp.empty( X_test.shape[0] , dtype=object))\n",
    "\n",
    "# create a dictionary to be dumped containing attribution and metadata\n",
    "results = {\n",
    "\t'attributions' : {},\n",
    "\t'segments' : segments,\n",
    "\t'y_test_true' : y_test,\n",
    "\t'y_test_pred' : preds,\n",
    "\t'label_mapping' : enc,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:21:34.526407Z",
     "start_time": "2024-06-11T15:21:34.523813Z"
    }
   },
   "outputs": [],
   "source": [
    "# define different background to be used and number of samples as n_background\n",
    "# TODO set a number of TOTAL sampling regardless of the background type?\n",
    "n_background = 50\n",
    "background_types = [\"average\", \"zero\",\"sampling\"] # zero, constant, average, multisample\n",
    "for bt in background_types:\n",
    "\tresults['attributions'][bt] = np.zeros( X_test.shape ,dtype=np.float32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:23:43.022377Z",
     "start_time": "2024-06-11T15:21:34.527062Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]c:\\Users\\Nikos\\miniconda3\\envs\\env_segment_shap\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Nikos\\miniconda3\\envs\\env_segment_shap\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\Nikos\\miniconda3\\envs\\env_segment_shap\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Nikos\\miniconda3\\envs\\env_segment_shap\\Lib\\site-packages\\numpy\\core\\_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "c:\\Users\\Nikos\\miniconda3\\envs\\env_segment_shap\\Lib\\site-packages\\numpy\\core\\_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 50%|█████     | 1/2 [00:13<00:13, 13.14s/it]c:\\Users\\Nikos\\miniconda3\\envs\\env_segment_shap\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Nikos\\miniconda3\\envs\\env_segment_shap\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\Nikos\\miniconda3\\envs\\env_segment_shap\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Nikos\\miniconda3\\envs\\env_segment_shap\\Lib\\site-packages\\numpy\\core\\_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "c:\\Users\\Nikos\\miniconda3\\envs\\env_segment_shap\\Lib\\site-packages\\numpy\\core\\_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 2/2 [00:26<00:00, 13.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 15 31 47 101 116] [15 31 47 101 116 150]\n",
      "[0 43 63 84 99 117] [43 63 84 99 117 150]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\t\n",
    "\tSHAP = ShapleyValueSampling(clf) if predictor_name in predictors['torch'] \\\n",
    "\t\telse ShapleyValueSampling(forward_classification)\n",
    "\t\n",
    "\tfor i in trange ( n_samples ) : #\n",
    "\t\t\n",
    "\t\t# get current sample and label\n",
    "\t\tts, y = X_test[i] , torch.tensor( y_test[i:i+1] )\n",
    "\n",
    "\t\t# get segment and its tensor representation\n",
    "\t\tcurrent_segments = segmentation_method(ts)[:X_test.shape[1]]\n",
    "\t\tresults['segments'][i] = current_segments\n",
    "\t\tmask = get_feature_mask(current_segments,ts.shape[-1])\n",
    "\n",
    "\t\tts = torch.tensor(ts).repeat(1,1,1)\t#TODO use something similar to np.expand_dim?\n",
    "\n",
    "\t\tfor background_type in background_types:\n",
    "\n",
    "\t\t\t# background data\n",
    "\t\t\tif background_type==\"zero\":\n",
    "\t\t\t\tbackground_dataset = torch.zeros((1,) + X_train.shape[1:])\n",
    "\t\t\telif background_type==\"sampling\":\n",
    "\t\t\t\tbackground_dataset = sample_background(X_train, n_background)\n",
    "\t\t\telif background_type==\"average\":\n",
    "\t\t\t\tbackground_dataset = sample_background(X_train, n_background).mean(axis=0, keepdim=True)\n",
    "\n",
    "\t\t\t# for sampling strategy repeat the ts many times as the background dataset size\n",
    "\t\t\tts = ts.repeat(background_dataset.shape[0],1,1) if background_type==\"sampling\" else ts\n",
    "\n",
    "\t\t\t# different call depending on predictor type\n",
    "\t\t\tif predictor_name in predictors['scikit']:\n",
    "\t\t\t\t# if using random forest flat everything\n",
    "\t\t\t\tif predictor_name==\"randomForest\":\n",
    "\t\t\t\t\tts = ts.reshape( -1, n_chs*ts_length); mask = mask.reshape( -1, n_chs*ts_length);\n",
    "\t\t\t\t\tbackground_dataset = background_dataset.reshape( -1, n_chs*ts_length)\n",
    "\t\t\t\t\n",
    "\t\t\t\ttmp = SHAP.attribute( ts, target=y , feature_mask=mask, baselines=background_dataset, additional_forward_args=clf)\n",
    "\t\t\t\n",
    "\t\t\telif predictor_name in predictors['torch']:\n",
    "\t\t\t\t# if use torch make sure everything is on selected device\n",
    "\t\t\t\tts = ts.to(device); y = y.to(device)\n",
    "\t\t\t\tmask = mask.to(device) ; background_dataset =  background_dataset.to(device)\n",
    "\t\t\t\ttmp = SHAP.attribute( ts, target=y , feature_mask=mask, baselines=background_dataset)\n",
    "\t\t\t\n",
    "\t\t\t# 'un-flatten' for randomForest\n",
    "\t\t\tif predictor_name==\"randomForest\":\n",
    "\t\t\t\ttmp = tmp.reshape(-1,X_test.shape[1],X_test.shape[2])\n",
    "\n",
    "\t\t\t# store current explanation in the data structure; if sampling store the mean\n",
    "\t\t\tresults['attributions'][background_type][i] = torch.mean(tmp, dim=0).cpu().numpy() if \\\n",
    "\t\t\t\tbackground_type==\"sampling\" else tmp[0].cpu().numpy()\n",
    "\n",
    "\tif segmentation_type==\"normalized\":\n",
    "\t\tweights = np.array(list(map(lambda x: list(map(lambda y: lengths_to_weights(change_points_to_lengths(y, X_train.shape[-1])), x)), results[\"segments\"])))\n",
    "\t\tfor background_type in background_types:\n",
    "\t\t\tresults[\"attributions\"][background_type] *= weights\n",
    "\n",
    "\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:08:49.634218Z",
     "start_time": "2024-06-11T15:08:49.630046Z"
    }
   },
   "outputs": [],
   "source": [
    "# dump result to disk\n",
    "file_name = \"_\".join ( ( predictor_name, dataset_name ) )+\".npy\"\n",
    "file_path = os.path.join(\"attributions\", file_name)\n",
    "np.save( file_path, results )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_segment_shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
