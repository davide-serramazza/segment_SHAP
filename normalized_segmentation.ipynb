{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# imports "
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T15:25:36.803749Z",
     "start_time": "2024-06-20T15:25:33.087329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from captum.attr import ShapleyValueSampling\n",
    "from tqdm import trange\n",
    "\n",
    "from load_data import load_data\n",
    "from train_models import *\n",
    "from segmentation import *\n",
    "from utils import *\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# device for torch"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T15:25:36.809817Z",
     "start_time": "2024-06-20T15:25:36.804776Z"
    }
   },
   "source": [
    "from torch.cuda import is_available as is_GPU_available\n",
    "device = \"cuda\" if is_GPU_available() else \"cpu\"\n",
    "\n",
    "# dictionary mapping predictors to torch vs other, step necessary for Captum \n",
    "predictors = {\n",
    "\t'torch' : ['resNet'],\n",
    "\t'scikit' : ['miniRocket','randomForest']\n",
    "}\n",
    "segmentation_dict = {\"clasp\":get_claSP_segmentation, \"infogain\": get_InformationGain_segmentation, \"greedygaussian\": get_GreedyGaussian_segmentation, \"equal\": get_equal_segmentation, \"nnsegment\": get_NNSegment_segmentation}"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# hyper-parameters"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:08:28.023540Z",
     "start_time": "2024-06-20T17:08:28.017441Z"
    }
   },
   "source": [
    "# hyperparameters\n",
    "n_background = 50\n",
    "\n",
    "# settings\n",
    "dataset_names = {'gunpointz'}\n",
    "predictor_names = {'randomForest'} # {\"randomForest\", 'miniRocket', 'resNet'}\n",
    "segmentation_names = {\"clasp\",\"greedygaussian\", \"equal\", \"nnsegment\"}  #, \"infogain\"\n",
    "segmentation_types = {\"default\", \"normalized\"}\n",
    "background_types = {\"average\", \"sampling\",\"zero\"} #{\"average\", \"zero\", \"sampling\"}\n",
    "\n",
    "# demo\n",
    "# dataset_names = {'gunpoint'}\n",
    "# predictor_names = {\"randomForest\"}\n",
    "# segmentation_names = {\"clasp\"}\n",
    "# segmentation_types = {\"default\", \"normalized\"}\n",
    "# background_types = {\"average\", \"zero\"}"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T12:58:40.243208Z",
     "start_time": "2024-06-20T12:58:40.240864Z"
    }
   },
   "source": [
    "results_demo = {\"dataset_name\":\n",
    "                    {' segments' :\n",
    "                            {\"segmentation_name\":\n",
    "                                {\"predictors\":\n",
    "                                        {\"predictor_name\":{\n",
    "                                                \"backgrounds\":{\n",
    "                                                    \"background_name\":{\n",
    "                                                            \"default\": None, \"normalized\": None},\n",
    "                                                },\n",
    "                                                'y_test_pred' : None,        \n",
    "                                        }, \n",
    "                                \"segments\": None},\n",
    "}\n",
    "                            },\n",
    "                    'y_test_true': None,\n",
    "                    'label_mapping': None}\n",
    "                        }"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# train model"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:08:32.265115Z",
     "start_time": "2024-06-20T17:08:31.756589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = dict()\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    # init dataset\n",
    "    # load data\n",
    "    X_train, X_test, y_train, y_test, enc = load_data(subset='all', dataset_name=dataset_name)\n",
    "    # for debugging only\n",
    "    X_test = X_test[:2]\n",
    "    y_test = y_test[:2]\n",
    "    n_samples, n_chs, ts_length = X_test.shape\n",
    "\n",
    "    results[dataset_name] = {'y_test_true': y_test, 'label_mapping':enc}\n",
    "\n",
    "    predictor_dict = dict()\n",
    "    for predictor_name in predictor_names:\n",
    "\n",
    "        if predictor_name=='resNet':\n",
    "            clf,preds = train_ResNet(X_train, y_train, X_test, y_test, dataset_name,device=device)\n",
    "        elif predictor_name=='miniRocket':\n",
    "            clf,preds = train_miniRocket(X_train, y_train, X_test, y_test, dataset_name)\n",
    "        elif predictor_name==\"randomForest\":\n",
    "            clf, preds = train_randomForest(X_train, y_train, X_test, y_test, dataset_name)\n",
    "        else:\n",
    "            raise ValueError(\"predictor not found\")\n",
    "\n",
    "        predictor_dict[predictor_name] = {\"clf\": clf, \"preds\": preds}\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training random forest\n",
      "random forest accuracy is 0.5\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:09:48.699440Z",
     "start_time": "2024-06-20T17:08:33.656211Z"
    }
   },
   "source": [
    "with torch.no_grad():\n",
    "    for dataset_name in dataset_names:\n",
    "    \n",
    "        for segmentation_name in segmentation_names:\n",
    "\n",
    "            init_segments = np.empty( (X_test.shape[0] , X_test.shape[1]), dtype=object) if X_test.shape[1] > 1  else (np.empty( X_test.shape[0] , dtype=object))\n",
    "            results[dataset_name][segmentation_name] = {\"segments\": init_segments}\n",
    "            for predictor_name in predictor_names:\n",
    "                results[dataset_name][segmentation_name][predictor_name] = {'attributions': dict()}\n",
    "            segmentation_method = segmentation_dict[segmentation_name]\n",
    "    \n",
    "            ts_list = []\n",
    "            mask_list = []\n",
    "            y_list = []\n",
    "    \n",
    "            for i in range(n_samples) : #\n",
    "                \n",
    "                # get current sample and label\n",
    "                ts, y = X_test[i] , torch.tensor( y_test[i:i+1] )\n",
    "    \n",
    "                # get segment and its tensor representation\n",
    "                current_segments = segmentation_method(ts)[:X_test.shape[1]]\n",
    "                results[dataset_name][segmentation_name]['segments'][i] = current_segments\n",
    "                mask = get_feature_mask(current_segments,ts.shape[-1])\n",
    "                mask_list.append(mask)\n",
    "                ts = torch.tensor(ts).repeat(1,1,1)\t#TODO use something similar to np.expand_dim?\n",
    "                ts_list.append(ts)\n",
    "                y_list.append(y)\n",
    "    \n",
    "            for background_type in background_types:\n",
    "    \n",
    "                # background data\n",
    "                if background_type==\"zero\":\n",
    "                    background_dataset = torch.zeros((1,) + X_train.shape[1:])\n",
    "                elif background_type==\"sampling\":\n",
    "                    background_dataset = sample_background(X_train, n_background)\n",
    "                elif background_type==\"average\":\n",
    "                    background_dataset = sample_background(X_train, n_background).mean(axis=0, keepdim=True)\n",
    "    \n",
    "                for predictor_name in predictor_names:\n",
    "    \n",
    "                    clf = predictor_dict[predictor_name][\"clf\"]\n",
    "                    results[dataset_name][segmentation_name][predictor_name]['y_test_pred'] = predictor_dict[predictor_name][\"preds\"]\n",
    "\n",
    "                    results[dataset_name][segmentation_name][predictor_name]['attributions'][background_type] = {\"default\": np.zeros( X_test.shape ,dtype=np.float32 )}\n",
    "                    \n",
    "                    SHAP = ShapleyValueSampling(clf) if predictor_name in predictors['torch'] \\\n",
    "                        else ShapleyValueSampling(forward_classification)\n",
    "\n",
    "                    #print (background_type, predictor_name , segmentation_name)\n",
    "\n",
    "                    with tqdm(total=len(ts_list)) as pbar:\n",
    "                        for i, (ts, mask, y) in enumerate(zip(ts_list, mask_list, y_list)):\n",
    "    \n",
    "                            # for sampling strategy repeat the ts many times as the background dataset size\n",
    "                            ts = ts.repeat(background_dataset.shape[0],1,1) if background_type==\"sampling\" else ts\n",
    "                            \n",
    "                            # different call depending on predictor type\n",
    "                            if predictor_name in predictors['scikit']:\n",
    "                                # if using random forest flat everything\n",
    "                                if predictor_name==\"randomForest\":\n",
    "                                    ts = ts.reshape( -1, n_chs*ts_length)\n",
    "                                    mask = mask.reshape( -1, n_chs*ts_length)\n",
    "                                    background_dataset = background_dataset.reshape( -1, n_chs*ts_length)\n",
    "                                \n",
    "                                tmp = SHAP.attribute( ts, target=y , feature_mask=mask, baselines=background_dataset, additional_forward_args=clf)\n",
    "                            \n",
    "                            elif predictor_name in predictors['torch']:\n",
    "                                # if use torch make sure everything is on selected device\n",
    "                                ts = ts.to(device); y = y.to(device)\n",
    "                                mask = mask.to(device) ; background_dataset =  background_dataset.to(device)\n",
    "                                tmp = SHAP.attribute( ts, target=y , feature_mask=mask, baselines=background_dataset)\n",
    "                            \n",
    "                            # 'un-flatten' for randomForest\n",
    "                            if predictor_name==\"randomForest\":\n",
    "                                tmp = tmp.reshape(-1,X_test.shape[1],X_test.shape[2])\n",
    "    \n",
    "                            # store current explanation in the data structure; if sampling store the mean\n",
    "                            results[dataset_name][segmentation_name][predictor_name]['attributions'][background_type][\"default\"][i] = torch.mean(tmp, dim=0).cpu().numpy() if \\\n",
    "                                background_type==\"sampling\" else tmp[0].cpu().numpy()\n",
    "                            \n",
    "                            # update tqdm\n",
    "                            pbar.update(1)\n",
    "                    \n",
    "                    pbar.close()\n",
    "\n",
    "                    if \"normalized\" in segmentation_types:\n",
    "                        weights = np.array(list(map(lambda x: list(map(lambda y: lengths_to_weights(change_points_to_lengths(y, X_train.shape[-1])), x)), results[dataset_name][segmentation_name][\"segments\"])))\n",
    "\n",
    "                        results[dataset_name][segmentation_name][predictor_name]['attributions'][background_type][\"normalized\"] = results[dataset_name][segmentation_name][predictor_name]['attributions'][background_type][\"default\"] * weights\n",
    "\n",
    "            \n",
    "    \n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davide/miniconda3/envs/segment_shap/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/davide/miniconda3/envs/segment_shap/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/davide/miniconda3/envs/segment_shap/lib/python3.11/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/davide/miniconda3/envs/segment_shap/lib/python3.11/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/home/davide/miniconda3/envs/segment_shap/lib/python3.11/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 50%|█████     | 1/2 [01:06<01:06, 66.94s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T13:27:20.262518Z",
     "start_time": "2024-06-20T13:27:20.256868Z"
    }
   },
   "cell_type": "code",
   "source": "results['UWAVE'].keys()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['y_test_true', 'label_mapping', 'clasp', 'infogain', 'greedygaussian', 'nnsegment', 'equal'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T12:51:30.077722Z",
     "start_time": "2024-06-20T12:51:30.074411Z"
    }
   },
   "cell_type": "code",
   "source": "X_train.shape",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1, 150)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T15:37:43.606835Z",
     "start_time": "2024-06-20T15:37:43.598674Z"
    }
   },
   "source": [
    "# dump result to disk\n",
    "file_name = \"all_results_resNet\" # \"_\".join ( ( predictor_name, dataset_name ) )+\".npy\"\n",
    "file_path = os.path.join(\"attributions\", file_name)\n",
    "np.save( file_path, results )"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD keep temporarily -  to be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:47:39.782779Z",
     "start_time": "2024-06-14T14:47:35.430030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ResNet\n",
      "Epoch 1: train loss:  0.699, \t train accuracy  0.440 \n",
      "          test loss:  0.694,  \t test accuracy  0.493\n",
      "Epoch 11: train loss:  0.688, \t train accuracy  0.520 \n",
      "          test loss:  0.691,  \t test accuracy  0.627\n",
      "Epoch 21: train loss:  0.628, \t train accuracy  0.640 \n",
      "          test loss:  0.627,  \t test accuracy  0.680\n",
      "Epoch 31: train loss:  0.518, \t train accuracy  0.780 \n",
      "          test loss:  0.588,  \t test accuracy  0.700\n",
      "Epoch 41: train loss:  0.447, \t train accuracy  0.800 \n",
      "          test loss:  0.561,  \t test accuracy  0.707\n",
      "Epoch 51: train loss:  0.489, \t train accuracy  0.800 \n",
      "          test loss:  0.568,  \t test accuracy  0.713\n",
      "Epoch 61: train loss:  0.444, \t train accuracy  0.800 \n",
      "          test loss:  0.551,  \t test accuracy  0.720\n",
      "Epoch 71: train loss:  0.442, \t train accuracy  0.760 \n",
      "          test loss:  0.551,  \t test accuracy  0.720\n",
      "Epoch 81: train loss:  0.391, \t train accuracy  0.820 \n",
      "          test loss:  0.521,  \t test accuracy  0.767\n",
      "Epoch 91: train loss:  0.480, \t train accuracy  0.780 \n",
      "          test loss:  0.544,  \t test accuracy  0.780\n",
      "Epoch 101: train loss:  0.431, \t train accuracy  0.780 \n",
      "          test loss:  0.544,  \t test accuracy  0.780\n",
      "Epoch 111: train loss:  0.403, \t train accuracy  0.800 \n",
      "          test loss:  0.544,  \t test accuracy  0.780\n",
      "Epoch 121: train loss:  0.330, \t train accuracy  0.900 \n",
      "          test loss:  0.438,  \t test accuracy  0.887\n",
      "Epoch 131: train loss:  0.354, \t train accuracy  0.900 \n",
      "          test loss:  0.288,  \t test accuracy  0.933\n",
      "Epoch 141: train loss:  0.031, \t train accuracy  1.000 \n",
      "          test loss:  0.158,  \t test accuracy  0.967\n",
      "accuracy for resNet is  0.9866666793823242\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # train model\n",
    "# predictor_name = 'resNet'\n",
    "# if predictor_name=='resNet':\n",
    "# \tclf,preds = train_ResNet(X_train, y_train, X_test, y_test, dataset_name,device=device)\n",
    "# elif predictor_name=='miniRocket':\n",
    "# \tclf,preds = train_miniRocket(X_train, y_train, X_test, y_test, dataset_name)\n",
    "# elif predictor_name==\"randomForest\":\n",
    "# \tclf, preds = train_randomForest(X_train, y_train, X_test, y_test, dataset_name)\n",
    "# else:\n",
    "# \traise ValueError(\n",
    "# \t\t\"predictor not found\"\n",
    "# \t)\n",
    "\n",
    "# # segmentation\n",
    "# segmentation_name = \"nnsegment\"\n",
    "# segmentation_type = \"normalized\"\n",
    "# segmentation_method = segmentation_dict[segmentation_name]\n",
    "\n",
    "# # initialize data structure meant to contain the segments\n",
    "# # TODO can I be cleaner here?\n",
    "# segments =  np.empty( (X_test.shape[0] , X_test.shape[1]), dtype=object) if X_test.shape[1] > 1  else (\n",
    "# \tnp.empty( X_test.shape[0] , dtype=object))\n",
    "\n",
    "# # create a dictionary to be dumped containing attribution and metadata\n",
    "# results = {\n",
    "# \t'attributions' : {},\n",
    "# \t'segments' : segments,\n",
    "# \t'y_test_true' : y_test,\n",
    "# \t'y_test_pred' : preds,\n",
    "# \t'label_mapping' : enc,\n",
    "# }\n",
    "\n",
    "# # result # dataset # model# segment # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:47:39.786209Z",
     "start_time": "2024-06-14T14:47:39.783554Z"
    }
   },
   "outputs": [],
   "source": [
    "# # define different background to be used and number of samples as n_background\n",
    "# # TODO set a number of TOTAL sampling regardless of the background type?\n",
    "# # zero, constant, average, multisample\n",
    "# for bt in background_types:\n",
    "# \tresults['attributions'][bt] = np.zeros( X_test.shape ,dtype=np.float32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "\t\n",
    "# \tSHAP = ShapleyValueSampling(clf) if predictor_name in predictors['torch'] \\\n",
    "# \t\telse ShapleyValueSampling(forward_classification)\n",
    "\t\n",
    "# \tfor i in trange ( n_samples ) : #\n",
    "\t\t\n",
    "# \t\t# get current sample and label\n",
    "# \t\tts, y = X_test[i] , torch.tensor( y_test[i:i+1] )\n",
    "\n",
    "# \t\t# get segment and its tensor representation\n",
    "# \t\tcurrent_segments = segmentation_method(ts)[:X_test.shape[1]]\n",
    "# \t\tresults['segments'][i] = current_segments\n",
    "# \t\tmask = get_feature_mask(current_segments,ts.shape[-1])\n",
    "\n",
    "# \t\tts = torch.tensor(ts).repeat(1,1,1)\t#TODO use something similar to np.expand_dim?\n",
    "\n",
    "# \t\tfor background_type in background_types:\n",
    "\n",
    "# \t\t\t# background data\n",
    "# \t\t\tif background_type==\"zero\":\n",
    "# \t\t\t\tbackground_dataset = torch.zeros((1,) + X_train.shape[1:])\n",
    "# \t\t\telif background_type==\"sampling\":\n",
    "# \t\t\t\tbackground_dataset = sample_background(X_train, n_background)\n",
    "# \t\t\telif background_type==\"average\":\n",
    "# \t\t\t\tbackground_dataset = sample_background(X_train, n_background).mean(axis=0, keepdim=True)\n",
    "\n",
    "# \t\t\t# for sampling strategy repeat the ts many times as the background dataset size\n",
    "# \t\t\tts = ts.repeat(background_dataset.shape[0],1,1) if background_type==\"sampling\" else ts\n",
    "\n",
    "# \t\t\t# different call depending on predictor type\n",
    "# \t\t\tif predictor_name in predictors['scikit']:\n",
    "# \t\t\t\t# if using random forest flat everything\n",
    "# \t\t\t\tif predictor_name==\"randomForest\":\n",
    "# \t\t\t\t\tts = ts.reshape( -1, n_chs*ts_length); mask = mask.reshape( -1, n_chs*ts_length);\n",
    "# \t\t\t\t\tbackground_dataset = background_dataset.reshape( -1, n_chs*ts_length)\n",
    "\t\t\t\t\n",
    "# \t\t\t\ttmp = SHAP.attribute( ts, target=y , feature_mask=mask, baselines=background_dataset, additional_forward_args=clf)\n",
    "\t\t\t\n",
    "# \t\t\telif predictor_name in predictors['torch']:\n",
    "# \t\t\t\t# if use torch make sure everything is on selected device\n",
    "# \t\t\t\tts = ts.to(device); y = y.to(device)\n",
    "# \t\t\t\tmask = mask.to(device) ; background_dataset =  background_dataset.to(device)\n",
    "# \t\t\t\ttmp = SHAP.attribute( ts, target=y , feature_mask=mask, baselines=background_dataset)\n",
    "\t\t\t\n",
    "# \t\t\t# 'un-flatten' for randomForest\n",
    "# \t\t\tif predictor_name==\"randomForest\":\n",
    "# \t\t\t\ttmp = tmp.reshape(-1,X_test.shape[1],X_test.shape[2])\n",
    "\n",
    "# \t\t\t# store current explanation in the data structure; if sampling store the mean\n",
    "# \t\t\tresults['attributions'][background_type][i] = torch.mean(tmp, dim=0).cpu().numpy() if \\\n",
    "# \t\t\t\tbackground_type==\"sampling\" else tmp[0].cpu().numpy()\n",
    "\n",
    "# \tif segmentation_type==\"normalized\":\n",
    "# \t\tweights = np.array(list(map(lambda x: list(map(lambda y: lengths_to_weights(change_points_to_lengths(y, X_train.shape[-1])), x)), results[\"segments\"])))\n",
    "# \t\tfor background_type in background_types:\n",
    "# \t\t\tresults[\"attributions\"][background_type] *= weights\n",
    "\n",
    "\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:08:49.634218Z",
     "start_time": "2024-06-11T15:08:49.630046Z"
    }
   },
   "outputs": [],
   "source": [
    "# # dump result to disk\n",
    "# file_name = \"_\".join ( ( predictor_name, dataset_name ) )+\".npy\"\n",
    "# file_path = os.path.join(\"attributions\", file_name)\n",
    "# np.save( file_path, results )"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T19:15:03.946084Z",
     "start_time": "2024-06-19T19:15:03.943728Z"
    }
   },
   "cell_type": "code",
   "source": "print (\"gpu available\",torch.cuda.is_available())",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu avaliable True\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_segment_shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
