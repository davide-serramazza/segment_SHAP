{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T07:52:03.121792Z",
     "start_time": "2024-07-10T07:52:02.892872Z"
    }
   },
   "cell_type": "code",
   "source": "%reset -f",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T07:52:03.434381Z",
     "start_time": "2024-07-10T07:52:03.428963Z"
    }
   },
   "source": [
    "from captum.attr import ShapleyValueSampling\n",
    "from tqdm import trange\n",
    "\n",
    "from load_data import load_data\n",
    "from train_models import *\n",
    "from segmentation import *\n",
    "from utils import *\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import timeit"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# device for torch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T07:52:04.274681Z",
     "start_time": "2024-07-10T07:52:04.271860Z"
    }
   },
   "source": [
    "from torch.cuda import is_available as is_GPU_available\n",
    "device = \"cuda\" if is_GPU_available() else \"cpu\"\n",
    "\n",
    "# dictionary mapping predictors to torch vs other, step necessary for Captum \n",
    "predictors = {\n",
    "\t'torch' : ['resNet'],\n",
    "\t'scikit' : ['miniRocket','randomForest']\n",
    "}\n",
    "segmentation_dict = {\"clasp\":get_claSP_segmentation, \"infogain\": get_InformationGain_segmentation, \"greedygaussian\": get_GreedyGaussian_segmentation, \"equal\": get_equal_segmentation, \"nnsegment\": get_NNSegment_segmentation}"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# hyper-parameters"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T07:52:17.785059Z",
     "start_time": "2024-07-10T07:52:17.782313Z"
    }
   },
   "source": [
    "# hyperparameters\n",
    "n_background = 50\n",
    "\n",
    "# settings\n",
    "dataset_names = {'MP'}    #{sys.argv[1]}\n",
    "predictor_names = {'randomForest'}    #{sys.argv[2]} {\"randomForest\", 'miniRocket', 'resNet'}\n",
    "segmentation_names = {\"clasp\",\"greedygaussian\", \"equal\", \"infogain\",\"nnsegment\"} #, \"nnsegment\"}  #, infogain , ,\n",
    "background_names =  {\"average\", \"zero\",\"sampling\"} #{\"average\", \"zero\", \"sampling\"}\n",
    "normalization_names = {\"default\", \"normalized\"}\n",
    "\n",
    "demo_mode = False\n",
    "# demo\n",
    "if demo_mode:\n",
    "    dataset_names = {'MP'}\n",
    "    predictor_names = {\"randomForest\"}\n",
    "    segmentation_names = {'clasp'}\n",
    "    background_names = {'zero','sampling'}\n",
    "    normalization_names = {\"default\", \"normalized\"}\n"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# instantiate the dictionary"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T07:52:18.955606Z",
     "start_time": "2024-07-10T07:52:18.951857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = dict.fromkeys(('y_test_true', 'label_mapping', \"segments\", 'y_test_pred', \"attributions\"))\n",
    "for key in results.keys():\n",
    "    results[key] = dict.fromkeys(dataset_names)\n",
    "normalization_names = normalization_names | {\"default\"}\n"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T07:52:27.556263Z",
     "start_time": "2024-07-10T07:52:20.256995Z"
    }
   },
   "source": [
    "for dataset_name in dataset_names:\n",
    "    # init dataset\n",
    "    # load data\n",
    "    X_train, X_test, y_train, y_test, enc = load_data(subset='all', dataset_name=dataset_name)\n",
    "    # for debugging only\n",
    "    if demo_mode:\n",
    "        X_test = X_test[:2]\n",
    "        y_test = y_test[:2]\n",
    "\n",
    "    n_samples, n_chs, ts_length = X_test.shape\n",
    "\n",
    "    results['y_test_true'][dataset_name] = y_test\n",
    "    results['label_mapping'][dataset_name] = enc\n",
    "    results[\"attributions\"][dataset_name] = dict.fromkeys(segmentation_names)\n",
    "    results[\"segments\"][dataset_name] = dict.fromkeys(segmentation_names)\n",
    "    results[\"y_test_pred\"][dataset_name] = dict.fromkeys(segmentation_names)\n",
    "\n",
    "    predictor_dict = dict()\n",
    "    # TODO not to save if in demo mode!\n",
    "    for predictor_name in predictor_names:\n",
    "        if demo_mode:\n",
    "            dataset_name=None\n",
    "\n",
    "        if predictor_name=='resNet':\n",
    "            clf,preds = train_ResNet(X_train, y_train, X_test, y_test, dataset_name,device=device)\n",
    "        elif predictor_name=='miniRocket':\n",
    "            clf,preds = train_miniRocket(X_train, y_train, X_test, y_test, dataset_name)\n",
    "        elif predictor_name==\"randomForest\":\n",
    "            clf, preds = train_randomForest(X_train, y_train, X_test, y_test, dataset_name)\n",
    "        else:\n",
    "            raise ValueError(\"predictor not found\")\n",
    "\n",
    "        predictor_dict[predictor_name] = {\"clf\": clf, \"preds\": preds}\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training random forest\n",
      "random forest accuracy is 0.4722689075630252\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T07:52:27.563975Z",
     "start_time": "2024-07-10T07:52:27.557306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_result_dict(X_test,predictor_names,dataset_name,segmentation_name,results):\n",
    "\tinit_segments = np.empty((X_test.shape[0], X_test.shape[1]), dtype=object) if X_test.shape[1] > 1 else (\n",
    "\t\tnp.empty(X_test.shape[0], dtype=object))\n",
    "\tresults[\"segments\"][dataset_name][segmentation_name] = init_segments.copy()\n",
    "\tresults[\"attributions\"][dataset_name][segmentation_name] = dict.fromkeys(predictor_names)\n",
    "\tresults[\"y_test_pred\"][dataset_name][segmentation_name] = dict.fromkeys(predictor_names)\n",
    "\tfor predictor_name in predictor_names:\n",
    "\t\tresults[\"attributions\"][dataset_name][segmentation_name][predictor_name] = dict.fromkeys(background_names)\n",
    "\n",
    "\n",
    "def get_sample_info(X_test,y_test,results, id):\n",
    "\t# get current sample and label\n",
    "\tts, y = X_test[id], torch.tensor(y_test[id:id + 1])\n",
    "\t# get segment and its tensor representation\n",
    "\tcurrent_segments = segmentation_method(ts)[:X_test.shape[1]]\n",
    "\tresults['segments'][dataset_name][segmentation_name][i] = current_segments\n",
    "\tmask = get_feature_mask(current_segments, ts.shape[-1])\n",
    "\tmask_list.append(mask)\n",
    "\tts = torch.tensor(ts).repeat(1, 1, 1)  # TODO use something similar to np.expand_dim?\n",
    "\tts_list.append(ts)\n",
    "\ty_list.append(y)\n",
    "\treturn ts,y,mask\n",
    "\n",
    "\n",
    "def get_background( background_name, results, normalization_names):\n",
    "\n",
    "\tresults[\"attributions\"][dataset_name][segmentation_name][predictor_name][background_name] = dict.fromkeys(\n",
    "\t\tnormalization_names)\n",
    "\t# background data\n",
    "\tif background_name == \"zero\":\n",
    "\t\tbackground_dataset = torch.zeros((1,) + X_train.shape[1:])\n",
    "\telif background_name == \"sampling\":\n",
    "\t\tbackground_dataset = sample_background(X_train, n_background)\n",
    "\telif background_name == \"average\":\n",
    "\t\tbackground_dataset = sample_background(X_train, n_background).mean(axis=0, keepdim=True)\n",
    "\n",
    "\treturn background_dataset\n",
    "\n",
    "\n",
    "def get_attribution(ts, mask, background_dataset,y, results ): #    global ts, mask, background_dataset, tmp, y\n",
    "\t# different call depending on predictor type\n",
    "\tif predictor_name in predictors['scikit']:\n",
    "\t\t# if using random forest flat everything\n",
    "\t\tif predictor_name == \"randomForest\":\n",
    "\t\t\tts = ts.reshape(-1, n_chs * ts_length)\n",
    "\t\t\tmask = mask.reshape(-1, n_chs * ts_length)\n",
    "\t\t\tbackground_dataset = background_dataset.reshape(-1, n_chs * ts_length)\n",
    "\n",
    "\t\ttmp = SHAP.attribute(ts, target=y, feature_mask=mask, baselines=background_dataset, additional_forward_args=clf)\n",
    "\n",
    "\telif predictor_name in predictors['torch']:\n",
    "\t\t# if use torch make sure everything is on selected device\n",
    "\t\tts = ts.to(device);\n",
    "\t\ty = y.to(device)\n",
    "\t\tmask = mask.to(device);\n",
    "\t\tbackground_dataset = background_dataset.to(device)\n",
    "\t\ttmp = SHAP.attribute(ts, target=y, feature_mask=mask, baselines=background_dataset)\n",
    "\n",
    "\t# in case of random forest 'un-flatten' result\n",
    "\tif predictor_name==\"randomForest\":\n",
    "\t\ttmp = tmp.reshape(-1,X_test.shape[1],X_test.shape[2])\n",
    "\n",
    "\t# lastly store current explanation in the data structure; if sampling store the mean\n",
    "\tresults['attributions'][dataset_name][segmentation_name][predictor_name][background_name][\"default\"][i] = torch.mean(tmp, dim=0).cpu().numpy() if \\\n",
    "\t\tbackground_name==\"sampling\" else tmp[0].cpu().numpy()\n",
    "\n",
    "\n",
    "def get_normalized_results(normalization_names,results):\n",
    "\tif \"normalized\" in normalization_names:\n",
    "\t\tweights = np.array(list(\n",
    "\t\t\tmap(lambda x: list(map(lambda y: lengths_to_weights(change_points_to_lengths(y, X_train.shape[-1])), x)),\n",
    "\t\t\t    results[\"segments\"][dataset_name][segmentation_name])))\n",
    "\n",
    "\t\tresults['attributions'][dataset_name][segmentation_name][predictor_name][background_name][\"normalized\"] = \\\n",
    "\t\t\tresults['attributions'][dataset_name][segmentation_name][predictor_name][background_name][\"default\"] * weights\n",
    "\tif \"default\" not in normalization_names:\n",
    "\t\tdel results['attributions'][dataset_name][segmentation_name][predictor_name][background_name][\"default\"]\n"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:04:10.437238Z",
     "start_time": "2024-07-10T08:04:05.695364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "starttime = timeit.default_timer()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset_name in dataset_names:\n",
    "        for segmentation_name in segmentation_names:\n",
    "            initialize_result_dict(X_test,predictor_names,dataset_name,segmentation_name,results)\n",
    "            segmentation_method = segmentation_dict[segmentation_name]\n",
    "            \n",
    "            ts_list = []\n",
    "            mask_list = []\n",
    "            y_list = []\n",
    "            \n",
    "            for i in range(n_samples) : \n",
    "                ts,y,mask = get_sample_info(X_test,y_test, results, i)\n",
    "            \n",
    "            for background_name in background_names:\n",
    "                background_dataset = get_background( background_name, results, normalization_names)\n",
    "                for predictor_name in predictor_names:\n",
    "                    # get clf and initialize attributions\n",
    "                    results['y_test_pred'][dataset_name][segmentation_name][predictor_name]=predictor_dict[predictor_name][\"preds\"]\n",
    "                    clf = predictor_dict[predictor_name][\"clf\"]\n",
    "                    init_attributions = np.zeros(X_test.shape, dtype=np.float32)\n",
    "                    for normalization_name in normalization_names:\n",
    "                        results['attributions'][dataset_name][segmentation_name][predictor_name][background_name][normalization_name] = init_attributions.copy()\n",
    "\n",
    "                    \n",
    "                    SHAP = ShapleyValueSampling(clf) if predictor_name in predictors['torch'] else ShapleyValueSampling(forward_classification)\n",
    "                    \n",
    "                    with tqdm(total=len(ts_list)) as pbar:\n",
    "                        for i, (ts, mask, y) in enumerate(zip(ts_list, mask_list, y_list)):\n",
    "                            # for sampling strategy repeat the ts many times as the background dataset size\n",
    "                            ts = ts.repeat(background_dataset.shape[0],1,1) if background_name==\"sampling\" else ts\n",
    "                            get_attribution (ts, mask, background_dataset,y, results )\n",
    "                            # update tqdm\n",
    "                            pbar.update(1)\n",
    "                            \n",
    "                    pbar.close()\n",
    "                    get_normalized_results(normalization_names,results)\n",
    "                    \n",
    "print(\"elapsed time\", ( timeit.default_timer() -starttime ) )\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 15\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_samples) : \n\u001B[1;32m     14\u001B[0m     \u001B[38;5;28mprint\u001B[39m(i)\n\u001B[0;32m---> 15\u001B[0m     ts,y,mask \u001B[38;5;241m=\u001B[39m \u001B[43mget_sample_info\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43my_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresults\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m background_name \u001B[38;5;129;01min\u001B[39;00m background_names:\n\u001B[1;32m     18\u001B[0m     background_dataset \u001B[38;5;241m=\u001B[39m get_background( background_name, results, normalization_names)\n",
      "Cell \u001B[0;32mIn[25], line 15\u001B[0m, in \u001B[0;36mget_sample_info\u001B[0;34m(X_test, y_test, results, id)\u001B[0m\n\u001B[1;32m     13\u001B[0m ts, y \u001B[38;5;241m=\u001B[39m X_test[\u001B[38;5;28mid\u001B[39m], torch\u001B[38;5;241m.\u001B[39mtensor(y_test[\u001B[38;5;28mid\u001B[39m:\u001B[38;5;28mid\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# get segment and its tensor representation\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m current_segments \u001B[38;5;241m=\u001B[39m \u001B[43msegmentation_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mts\u001B[49m\u001B[43m)\u001B[49m[:X_test\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]]\n\u001B[1;32m     16\u001B[0m results[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msegments\u001B[39m\u001B[38;5;124m'\u001B[39m][dataset_name][segmentation_name][i] \u001B[38;5;241m=\u001B[39m current_segments\n\u001B[1;32m     17\u001B[0m mask \u001B[38;5;241m=\u001B[39m get_feature_mask(current_segments, ts\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[0;32m~/workspace/PhD/bristolSHAP/segmentation.py:34\u001B[0m, in \u001B[0;36mget_claSP_segmentation\u001B[0;34m(X)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]):\n\u001B[1;32m     33\u001B[0m \tuts \u001B[38;5;241m=\u001B[39m X[ch]\n\u001B[0;32m---> 34\u001B[0m \tfound_cps, profiles, scores \u001B[38;5;241m=\u001B[39m \u001B[43mclasp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_clasp\u001B[49m\u001B[43m(\u001B[49m\u001B[43muts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m \t\u001B[38;5;66;03m# it seems that they are not sorted\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \tfound_cps\u001B[38;5;241m.\u001B[39msort()\n",
      "File \u001B[0;32m~/miniconda3/envs/segment_shap/lib/python3.11/site-packages/aeon/segmentation/_clasp.py:269\u001B[0m, in \u001B[0;36mClaSPSegmenter._run_clasp\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_clasp\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[1;32m    265\u001B[0m     clasp_transformer \u001B[38;5;241m=\u001B[39m ClaSPTransformer(\n\u001B[1;32m    266\u001B[0m         window_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mperiod_length, exclusion_radius\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexclusion_radius\n\u001B[1;32m    267\u001B[0m     )\u001B[38;5;241m.\u001B[39mfit(X)\n\u001B[0;32m--> 269\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfound_cps, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprofiles, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscores \u001B[38;5;241m=\u001B[39m \u001B[43m_segmentation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclasp_transformer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_change_points\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_cps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexclusion_radius\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexclusion_radius\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfound_cps, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprofiles, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscores\n",
      "File \u001B[0;32m~/miniconda3/envs/segment_shap/lib/python3.11/site-packages/aeon/segmentation/_clasp.py:147\u001B[0m, in \u001B[0;36m_segmentation\u001B[0;34m(X, clasp, n_change_points, exclusion_radius)\u001B[0m\n\u001B[1;32m    145\u001B[0m exclusion_zone \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mint64(\u001B[38;5;28mlen\u001B[39m(ranges) \u001B[38;5;241m*\u001B[39m exclusion_radius)\n\u001B[1;32m    146\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(ranges) \u001B[38;5;241m-\u001B[39m period_size \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m exclusion_zone:\n\u001B[0;32m--> 147\u001B[0m     profile \u001B[38;5;241m=\u001B[39m \u001B[43mclasp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[43mranges\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    148\u001B[0m     change_point \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(profile)\n\u001B[1;32m    149\u001B[0m     score \u001B[38;5;241m=\u001B[39m profile[change_point]\n",
      "File \u001B[0;32m~/miniconda3/envs/segment_shap/lib/python3.11/site-packages/aeon/transformations/base.py:444\u001B[0m, in \u001B[0;36mBaseTransformer.transform\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    441\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_is_fitted()\n\u001B[1;32m    443\u001B[0m \u001B[38;5;66;03m# input check and conversion for X/y\u001B[39;00m\n\u001B[0;32m--> 444\u001B[0m X_inner, y_inner, metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_X_y\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    446\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(X_inner, _VectorizedDF):\n\u001B[1;32m    447\u001B[0m     Xt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transform(X\u001B[38;5;241m=\u001B[39mX_inner, y\u001B[38;5;241m=\u001B[39my_inner)\n",
      "File \u001B[0;32m~/miniconda3/envs/segment_shap/lib/python3.11/site-packages/aeon/transformations/base.py:803\u001B[0m, in \u001B[0;36mBaseTransformer._check_X_y\u001B[0;34m(self, X, y, return_metadata)\u001B[0m\n\u001B[1;32m    801\u001B[0m \u001B[38;5;66;03m# retrieve supported mtypes\u001B[39;00m\n\u001B[1;32m    802\u001B[0m X_inner_type \u001B[38;5;241m=\u001B[39m _coerce_to_list(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_tag(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX_inner_type\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m--> 803\u001B[0m y_inner_type \u001B[38;5;241m=\u001B[39m _coerce_to_list(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_tag\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43my_inner_type\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    804\u001B[0m X_inner_abstract_type \u001B[38;5;241m=\u001B[39m abstract_types(X_inner_type)\n\u001B[1;32m    805\u001B[0m y_inner_abstract_type \u001B[38;5;241m=\u001B[39m abstract_types(y_inner_type)\n",
      "File \u001B[0;32m~/miniconda3/envs/segment_shap/lib/python3.11/site-packages/aeon/base/_base.py:390\u001B[0m, in \u001B[0;36mBaseObject.get_tag\u001B[0;34m(self, tag_name, tag_value_default, raise_error)\u001B[0m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_tag\u001B[39m(\u001B[38;5;28mself\u001B[39m, tag_name, tag_value_default\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, raise_error\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m    352\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    353\u001B[0m \u001B[38;5;124;03m    Get tag value from estimator class.\u001B[39;00m\n\u001B[1;32m    354\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    388\u001B[0m \u001B[38;5;124;03m    True\u001B[39;00m\n\u001B[1;32m    389\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 390\u001B[0m     collected_tags \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_tags\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    392\u001B[0m     tag_value \u001B[38;5;241m=\u001B[39m collected_tags\u001B[38;5;241m.\u001B[39mget(tag_name, tag_value_default)\n\u001B[1;32m    394\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m raise_error \u001B[38;5;129;01mand\u001B[39;00m tag_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m collected_tags\u001B[38;5;241m.\u001B[39mkeys():\n",
      "File \u001B[0;32m~/miniconda3/envs/segment_shap/lib/python3.11/site-packages/aeon/base/_base.py:344\u001B[0m, in \u001B[0;36mBaseObject.get_tags\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    319\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_tags\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    320\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    321\u001B[0m \u001B[38;5;124;03m    Get tags from estimator class.\u001B[39;00m\n\u001B[1;32m    322\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    342\u001B[0m \u001B[38;5;124;03m    >>> tags = d.get_tags()\u001B[39;00m\n\u001B[1;32m    343\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 344\u001B[0m     collected_tags \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_class_tags\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    346\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_tags_dynamic\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    347\u001B[0m         collected_tags\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tags_dynamic)\n",
      "File \u001B[0;32m~/miniconda3/envs/segment_shap/lib/python3.11/site-packages/aeon/base/_base.py:283\u001B[0m, in \u001B[0;36mBaseObject.get_class_tags\u001B[0;34m(cls)\u001B[0m\n\u001B[1;32m    280\u001B[0m         more_tags \u001B[38;5;241m=\u001B[39m parent_class\u001B[38;5;241m.\u001B[39m_tags\n\u001B[1;32m    281\u001B[0m         collected_tags\u001B[38;5;241m.\u001B[39mupdate(more_tags)\n\u001B[0;32m--> 283\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdeepcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcollected_tags\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/segment_shap/lib/python3.11/copy.py:146\u001B[0m, in \u001B[0;36mdeepcopy\u001B[0;34m(x, memo, _nil)\u001B[0m\n\u001B[1;32m    144\u001B[0m copier \u001B[38;5;241m=\u001B[39m _deepcopy_dispatch\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mcls\u001B[39m)\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m copier \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 146\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[43mcopier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    148\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28missubclass\u001B[39m(\u001B[38;5;28mcls\u001B[39m, \u001B[38;5;28mtype\u001B[39m):\n",
      "File \u001B[0;32m~/miniconda3/envs/segment_shap/lib/python3.11/copy.py:231\u001B[0m, in \u001B[0;36m_deepcopy_dict\u001B[0;34m(x, memo, deepcopy)\u001B[0m\n\u001B[1;32m    229\u001B[0m memo[\u001B[38;5;28mid\u001B[39m(x)] \u001B[38;5;241m=\u001B[39m y\n\u001B[1;32m    230\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m x\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m--> 231\u001B[0m     y[deepcopy(key, memo)] \u001B[38;5;241m=\u001B[39m \u001B[43mdeepcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m y\n",
      "File \u001B[0;32m~/miniconda3/envs/segment_shap/lib/python3.11/copy.py:137\u001B[0m, in \u001B[0;36mdeepcopy\u001B[0;34m(x, memo, _nil)\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m memo \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    135\u001B[0m     memo \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m--> 137\u001B[0m d \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mid\u001B[39m(x)\n\u001B[1;32m    138\u001B[0m y \u001B[38;5;241m=\u001B[39m memo\u001B[38;5;241m.\u001B[39mget(d, _nil)\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _nil:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T07:16:35.579738Z",
     "start_time": "2024-07-10T07:16:35.577461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dump result to disk\n",
    "if not demo_mode:\n",
    "    file_name = \"_\".join( (\"all_results\",dataset_name,predictor_name) )\n",
    "else:\n",
    "\tfile_name = \"_\".join( (\"all_results_DEMO_\",dataset_name,predictor_name) )\n",
    "file_path = os.path.join(\"attributions\", file_name)\n",
    "np.save( file_path, results )"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD keep temporarily -  to be deleted"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T15:31:24.960493Z",
     "start_time": "2024-07-01T15:31:24.957934Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 8,
   "source": [
    "# reference dict!\n",
    "results = {'y_test_true':\n",
    "                {\"dataset_name\":None},\n",
    "            'label_mapping': \n",
    "                {\"dataset_name\":None},\n",
    "            \"segments\":\n",
    "                {\"dataset_name\":\n",
    "                    {\"segmentation_name\":None}\n",
    "                },\n",
    "            'y_test_pred':\n",
    "                {\"dataset_name\":\n",
    "                    {\"segmentation_name\":\n",
    "                        {\"predictor_name\": None}\n",
    "                    }\n",
    "                },\n",
    "            \"attributions\":\n",
    "                {\"dataset_name\":\n",
    "                    {\"segmentation_name\":\n",
    "                        {\"predictor_name\":\n",
    "                            {\"background_name\":\n",
    "                                {\"normalization_name\": None}\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:38<00:00,  3.89it/s]\n",
      "100%|██████████| 150/150 [00:36<00:00,  4.16it/s]\n",
      "100%|██████████| 150/150 [00:36<00:00,  4.11it/s]\n",
      "100%|██████████| 150/150 [00:37<00:00,  3.97it/s]\n",
      "100%|██████████| 150/150 [00:35<00:00,  4.19it/s]\n",
      "100%|██████████| 150/150 [00:35<00:00,  4.18it/s]\n",
      "100%|██████████| 150/150 [00:32<00:00,  4.64it/s]\n",
      "100%|██████████| 150/150 [00:29<00:00,  5.01it/s]\n",
      "100%|██████████| 150/150 [00:30<00:00,  4.86it/s]\n",
      "100%|██████████| 150/150 [00:38<00:00,  3.89it/s]\n",
      "100%|██████████| 150/150 [00:36<00:00,  4.08it/s]\n",
      "100%|██████████| 150/150 [00:35<00:00,  4.17it/s]\n",
      "/home/davide/miniconda3/envs/segment_shap/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/davide/miniconda3/envs/segment_shap/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/davide/miniconda3/envs/segment_shap/lib/python3.11/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/davide/miniconda3/envs/segment_shap/lib/python3.11/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/home/davide/miniconda3/envs/segment_shap/lib/python3.11/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 150/150 [00:39<00:00,  3.79it/s]\n",
      "/home/davide/workspace/PhD/bristolSHAP/utils.py:145: RuntimeWarning: divide by zero encountered in divide\n",
      "  segment_weights = 1 / lengths\n",
      "100%|██████████| 150/150 [00:36<00:00,  4.14it/s]\n",
      "/home/davide/workspace/PhD/bristolSHAP/utils.py:145: RuntimeWarning: divide by zero encountered in divide\n",
      "  segment_weights = 1 / lengths\n",
      "100%|██████████| 150/150 [00:35<00:00,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time 575.0744022950003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/davide/workspace/PhD/bristolSHAP/utils.py:145: RuntimeWarning: divide by zero encountered in divide\n",
      "  segment_weights = 1 / lengths\n"
     ]
    }
   ],
   "execution_count": 10,
   "source": [
    "starttime = timeit.default_timer()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset_name in dataset_names:\n",
    "    \n",
    "        for segmentation_name in segmentation_names:\n",
    "\n",
    "            init_segments = np.empty( (X_test.shape[0] , X_test.shape[1]), dtype=object) if X_test.shape[1] > 1  else (np.empty( X_test.shape[0] , dtype=object))\n",
    "            results[\"segments\"][dataset_name][segmentation_name] = init_segments.copy()\n",
    "            results[\"attributions\"][dataset_name][segmentation_name] = dict.fromkeys(predictor_names)\n",
    "            results[\"y_test_pred\"][dataset_name][segmentation_name] = dict.fromkeys(predictor_names)\n",
    "            for predictor_name in predictor_names:\n",
    "                results[\"attributions\"][dataset_name][segmentation_name][predictor_name] = dict.fromkeys(background_names)\n",
    "            segmentation_method = segmentation_dict[segmentation_name]\n",
    "    \n",
    "            ts_list = []\n",
    "            mask_list = []\n",
    "            y_list = []\n",
    "    \n",
    "            for i in range(n_samples) : #\n",
    "                \n",
    "                # get current sample and label\n",
    "                ts, y = X_test[i] , torch.tensor( y_test[i:i+1] )\n",
    "    \n",
    "                # get segment and its tensor representation\n",
    "                current_segments = segmentation_method(ts)[:X_test.shape[1]]\n",
    "                results['segments'][dataset_name][segmentation_name][i] = current_segments\n",
    "                mask = get_feature_mask(current_segments,ts.shape[-1])\n",
    "                mask_list.append(mask)\n",
    "                ts = torch.tensor(ts).repeat(1,1,1)\t#TODO use something similar to np.expand_dim?\n",
    "                ts_list.append(ts)\n",
    "                y_list.append(y)\n",
    "    \n",
    "            for background_name in background_names:\n",
    "    \n",
    "                results[\"attributions\"][dataset_name][segmentation_name][predictor_name][background_name] = dict.fromkeys(normalization_names)\n",
    "                # background data\n",
    "                if background_name==\"zero\":\n",
    "                    background_dataset = torch.zeros((1,) + X_train.shape[1:])\n",
    "                elif background_name==\"sampling\":\n",
    "                    background_dataset = sample_background(X_train, n_background)\n",
    "                elif background_name==\"average\":\n",
    "                    background_dataset = sample_background(X_train, n_background).mean(axis=0, keepdim=True)\n",
    "    \n",
    "                for predictor_name in predictor_names:\n",
    "    \n",
    "                    clf = predictor_dict[predictor_name][\"clf\"]\n",
    "                    results['y_test_pred'][dataset_name][segmentation_name][predictor_name] = predictor_dict[predictor_name][\"preds\"]\n",
    "\n",
    "                    init_attributions = np.zeros(X_test.shape, dtype=np.float32)\n",
    "                    for normalization_name in normalization_names:\n",
    "                        results['attributions'][dataset_name][segmentation_name][predictor_name][background_name][normalization_name] = init_attributions.copy()\n",
    "                    \n",
    "                    SHAP = ShapleyValueSampling(clf) if predictor_name in predictors['torch'] \\\n",
    "                        else ShapleyValueSampling(forward_classification)\n",
    "\n",
    "                    #print (background_name, predictor_name , segmentation_name)\n",
    "\n",
    "                    with tqdm(total=len(ts_list)) as pbar:\n",
    "                        for i, (ts, mask, y) in enumerate(zip(ts_list, mask_list, y_list)):\n",
    "    \n",
    "                            # for sampling strategy repeat the ts many times as the background dataset size\n",
    "                            ts = ts.repeat(background_dataset.shape[0],1,1) if background_name==\"sampling\" else ts\n",
    "                            \n",
    "                            # different call depending on predictor type\n",
    "                            if predictor_name in predictors['scikit']:\n",
    "                                # if using random forest flat everything\n",
    "                                if predictor_name==\"randomForest\":\n",
    "                                    ts = ts.reshape( -1, n_chs*ts_length)\n",
    "                                    mask = mask.reshape( -1, n_chs*ts_length)\n",
    "                                    background_dataset = background_dataset.reshape( -1, n_chs*ts_length)\n",
    "                                \n",
    "                                tmp = SHAP.attribute( ts, target=y , feature_mask=mask, baselines=background_dataset, additional_forward_args=clf)\n",
    "                            \n",
    "                            elif predictor_name in predictors['torch']:\n",
    "                                # if use torch make sure everything is on selected device\n",
    "                                ts = ts.to(device); y = y.to(device)\n",
    "                                mask = mask.to(device) ; background_dataset =  background_dataset.to(device)\n",
    "                                tmp = SHAP.attribute( ts, target=y , feature_mask=mask, baselines=background_dataset)\n",
    "                            \n",
    "                            # 'un-flatten' for randomForest\n",
    "                            if predictor_name==\"randomForest\":\n",
    "                                tmp = tmp.reshape(-1,X_test.shape[1],X_test.shape[2])\n",
    "    \n",
    "                            # store current explanation in the data structure; if sampling store the mean\n",
    "                            results['attributions'][dataset_name][segmentation_name][predictor_name][background_name][\"default\"][i] = torch.mean(tmp, dim=0).cpu().numpy() if \\\n",
    "                                background_name==\"sampling\" else tmp[0].cpu().numpy()\n",
    "                            \n",
    "                            # update tqdm\n",
    "                            pbar.update(1)\n",
    "                    \n",
    "                    pbar.close()\n",
    "\n",
    "                    if \"normalized\" in normalization_names:\n",
    "                        weights = np.array(list(map(lambda x: list(map(lambda y: lengths_to_weights(change_points_to_lengths(y, X_train.shape[-1])), x)), results[\"segments\"][dataset_name][segmentation_name])))\n",
    "\n",
    "                        results['attributions'][dataset_name][segmentation_name][predictor_name][background_name][\"normalized\"] = results['attributions'][dataset_name][segmentation_name][predictor_name][background_name][\"default\"] * weights\n",
    "                    if \"default\" not in normalization_names:\n",
    "                        del results['attributions'][dataset_name][segmentation_name][predictor_name][background_name][\"default\"]\n",
    "                        \n",
    "print(\"elapsed time\", ( timeit.default_timer() -starttime ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_segment_shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
