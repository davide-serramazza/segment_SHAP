{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nikos\\miniconda3\\envs\\env_segment_shap\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from captum.attr import ShapleyValueSampling\n",
    "from tqdm import trange\n",
    "\n",
    "from load_data import load_data\n",
    "from train_models import train_randomForest\n",
    "from segmentation import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to utils.py\n",
    "\n",
    "def change_points_to_lengths(change_points, max_length):\n",
    "    # change points is 1D iterable of idxs\n",
    "    change_points = list(change_points)\n",
    "    start_points = [0] + change_points\n",
    "    end_points = change_points + [max_length]\n",
    "    lengths = np.array(end_points) - np.array(start_points)\n",
    "    return lengths\n",
    "\n",
    "def lengths_to_weights(lengths):\n",
    "    # lengths is 1D iterable of positive ints\n",
    "    start_idx = 0\n",
    "    end_idx = 0\n",
    "    segment_weights = 1 / lengths\n",
    "    weights = np.ones(lengths.sum())\n",
    "    for segment_weight, length in zip(segment_weights, lengths):\n",
    "        end_idx += length\n",
    "        weights[start_idx: end_idx] = segment_weight\n",
    "        start_idx = end_idx\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest accuracy is 0.5\n",
      "\n",
      " explaining sample n. 0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " explaining sample n. 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.57s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load data\n",
    "dataset_name = 'UWAVE' #\"gunpoint\" #\n",
    "X_train, X_test, y_train, y_test = load_data(subset='all', dataset_name=dataset_name)\n",
    "\n",
    "# X_test = X_test[:2]\n",
    "# y_test = y_test[:2]\n",
    "\n",
    "# train model\n",
    "clf, preds = train_randomForest(X_train,y_train,X_test,y_test, dataset_name)\n",
    "\n",
    "# create a dictionary to be dumped containing attribution and metadata\n",
    "# initialize data structure meant to contain the segments\n",
    "segments =  np.empty( (X_test.shape[0] , X_test.shape[1]), dtype=object) if X_test.shape[1] > 1  else (\n",
    "    np.empty( X_test.shape[0] , dtype=object))\n",
    "\n",
    "all_attributions = {\n",
    "    'attributions' : np.empty( X_test.shape ,dtype=np.float32 ),\n",
    "    'segments' : segments,\n",
    "    'y_test_true' : y_test,\n",
    "    'y_test_pred' : preds\n",
    "}\n",
    "\n",
    "# explain\n",
    "n_background = 50\n",
    "background_type = \"average\" # zero, constant, average, multisample\n",
    "batch_size = 32\n",
    "with torch.no_grad():\n",
    "    SHAP = ShapleyValueSampling(forward_classification)\n",
    "    for i in range ( X_test.shape[0] ) : # \n",
    "        # get current sample and label\n",
    "        ts, y = X_test[i] , torch.tensor( y_test[i:i+1] )\n",
    "\n",
    "        # get segment and its tensor representation\n",
    "        current_segments = get_claSP_segmentation(ts)[:X_test.shape[1]]\n",
    "        all_attributions['segments'][i] = current_segments\n",
    "        mask = get_feature_mask(current_segments,ts.shape[-1])\n",
    "\n",
    "        # background data\n",
    "        \n",
    "        if background_type==\"zero\":\n",
    "            background_dataset = torch.zeros((1,) + X_train.shape[1:])\n",
    "        elif background_type==\"sampling\":\n",
    "            background_dataset = sample_background(X_train, n_background)\n",
    "        elif background_type==\"average\":\n",
    "            background_dataset = sample_background(X_train, n_background).mean(axis=0, keepdim=True)\n",
    "\n",
    "        print(\"\\n explaining sample n.\",i,\"\\n\")\n",
    "        # data structure with room for each sample in the background dataset\n",
    "        current_attr = torch.zeros(background_dataset.shape[0], ts.shape[0], ts.shape[1])\n",
    "        for j in trange(0,background_dataset.shape[0] ,batch_size):\n",
    "\n",
    "            sample = background_dataset[j:j+batch_size]\n",
    "            actual_size = sample.shape[0]\n",
    "            batched_ts = torch.tensor( np.array([ts]*actual_size) )\n",
    "\n",
    "            ##### only for random forest as every instance should be a 1D tensor #######\n",
    "            batched_ts , sample = batched_ts.reshape(actual_size,-1), sample.reshape(actual_size,-1)\n",
    "            mask = mask.reshape(1,-1)\n",
    "            ###############################################################################\n",
    "\n",
    "            tmp = SHAP.attribute( batched_ts, target=y , feature_mask=mask, baselines=sample, additional_forward_args=clf)\n",
    "\n",
    "            ########  only for random forest as every instance should be a 1D tensor    ########\n",
    "            current_attr[j:j+actual_size] = tmp.reshape(actual_size,X_test.shape[1],X_test.shape[2])\n",
    "            ###############################################################################\n",
    "\n",
    "        # compute as final explanation mean of each explanation using a different baseline\n",
    "        all_attributions['attributions'][i] =torch.mean(current_attr,dim=0)\n",
    "\n",
    "weights = np.array(list(map(lambda x: list(map(lambda y: lengths_to_weights(change_points_to_lengths(y, X_train.shape[-1])), x)), all_attributions[\"segments\"])))\n",
    "all_attributions[\"attributions\"] *= weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7299999 , 0.16000003], dtype=float32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_attributions['attributions'].sum(axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_segment_shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
