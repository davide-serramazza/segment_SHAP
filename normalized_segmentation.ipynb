{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:46:18.928809Z",
     "start_time": "2024-06-14T14:46:16.742066Z"
    }
   },
   "source": [
    "from captum.attr import ShapleyValueSampling\n",
    "from tqdm import trange\n",
    "\n",
    "from load_data import load_data\n",
    "from train_models import *\n",
    "from segmentation import *\n",
    "from utils import *\n",
    "import torch\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:47:34.371228Z",
     "start_time": "2024-06-14T14:47:34.368207Z"
    }
   },
   "source": [
    "# device for torch\n",
    "from torch.cuda import is_available as is_GPU_available\n",
    "device = \"cuda\" if is_GPU_available() else \"cpu\"\n",
    "\n",
    "# dictionary mapping predictors to torch vs other, step necessary for Captum \n",
    "predictors = {\n",
    "\t'torch' : ['resNet'],\n",
    "\t'scikit' : ['miniRocket','randomForest']\n",
    "}\n",
    "\n",
    "segmentation_dict = {\"clasp\":get_claSP_segmentation, \"infogain\": get_InformationGain_segmentation, \"greedygaussian\": get_GreedyGaussian_segmentation, \"equal\": get_equal_segmentation, \"nnsegment\": get_NNSegment_segmentation}\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:47:34.879773Z",
     "start_time": "2024-06-14T14:47:34.876321Z"
    }
   },
   "source": [
    "# settings / hyperparameters\n",
    "\n",
    "predictor_names = {\"rf\", \"mlmodel\"}\n",
    "segmentations = {\"clasp\", \"infogain\", \"greedygaussian\", \"equal\", \"nnsegment\"}\n",
    "segmentation_types = {\"default\", \"normalized\"}\n",
    "background_types = {\"average\", \"zero\"} # \"sampling\"\n",
    "\n",
    "# hyperparameter {(segmentation, dataset): kwargs}"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:47:39.782779Z",
     "start_time": "2024-06-14T14:47:35.430030Z"
    }
   },
   "source": [
    "# load data\n",
    "dataset_name = 'gunpoint'\n",
    "X_train, X_test, y_train, y_test, enc = load_data(subset='all', dataset_name=dataset_name)\n",
    "# for debugging only\n",
    "# X_test = X_test[:2]\n",
    "# y_test = y_test[:2]\n",
    "n_samples, n_chs, ts_length = X_test.shape\n",
    "\n",
    "# train model\n",
    "predictor_name = 'resNet'\n",
    "if predictor_name=='resNet':\n",
    "\tclf,preds = train_ResNet(X_train, y_train, X_test, y_test, dataset_name,device=device)\n",
    "elif predictor_name=='miniRocket':\n",
    "\tclf,preds = train_miniRocket(X_train, y_train, X_test, y_test, dataset_name)\n",
    "elif predictor_name==\"randomForest\":\n",
    "\tclf, preds = train_randomForest(X_train, y_train, X_test, y_test, dataset_name)\n",
    "else:\n",
    "\traise ValueError(\n",
    "\t\t\"predictor not found\"\n",
    "\t)\n",
    "\n",
    "# segmentation\n",
    "segmentation_name = \"nnsegment\"\n",
    "segmentation_type = \"normalized\"\n",
    "segmentation_method = segmentation_dict[segmentation_name]\n",
    "\n",
    "# initialize data structure meant to contain the segments\n",
    "# TODO can I be cleaner here?\n",
    "segments =  np.empty( (X_test.shape[0] , X_test.shape[1]), dtype=object) if X_test.shape[1] > 1  else (\n",
    "\tnp.empty( X_test.shape[0] , dtype=object))\n",
    "\n",
    "# create a dictionary to be dumped containing attribution and metadata\n",
    "results = {\n",
    "\t'attributions' : {},\n",
    "\t'segments' : segments,\n",
    "\t'y_test_true' : y_test,\n",
    "\t'y_test_pred' : preds,\n",
    "\t'label_mapping' : enc,\n",
    "}\n",
    "\n",
    "# result # dataset # model# segment # "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ResNet\n",
      "Epoch 1: train loss:  0.699, \t train accuracy  0.440 \n",
      "          test loss:  0.694,  \t test accuracy  0.493\n",
      "Epoch 11: train loss:  0.688, \t train accuracy  0.520 \n",
      "          test loss:  0.691,  \t test accuracy  0.627\n",
      "Epoch 21: train loss:  0.628, \t train accuracy  0.640 \n",
      "          test loss:  0.627,  \t test accuracy  0.680\n",
      "Epoch 31: train loss:  0.518, \t train accuracy  0.780 \n",
      "          test loss:  0.588,  \t test accuracy  0.700\n",
      "Epoch 41: train loss:  0.447, \t train accuracy  0.800 \n",
      "          test loss:  0.561,  \t test accuracy  0.707\n",
      "Epoch 51: train loss:  0.489, \t train accuracy  0.800 \n",
      "          test loss:  0.568,  \t test accuracy  0.713\n",
      "Epoch 61: train loss:  0.444, \t train accuracy  0.800 \n",
      "          test loss:  0.551,  \t test accuracy  0.720\n",
      "Epoch 71: train loss:  0.442, \t train accuracy  0.760 \n",
      "          test loss:  0.551,  \t test accuracy  0.720\n",
      "Epoch 81: train loss:  0.391, \t train accuracy  0.820 \n",
      "          test loss:  0.521,  \t test accuracy  0.767\n",
      "Epoch 91: train loss:  0.480, \t train accuracy  0.780 \n",
      "          test loss:  0.544,  \t test accuracy  0.780\n",
      "Epoch 101: train loss:  0.431, \t train accuracy  0.780 \n",
      "          test loss:  0.544,  \t test accuracy  0.780\n",
      "Epoch 111: train loss:  0.403, \t train accuracy  0.800 \n",
      "          test loss:  0.544,  \t test accuracy  0.780\n",
      "Epoch 121: train loss:  0.330, \t train accuracy  0.900 \n",
      "          test loss:  0.438,  \t test accuracy  0.887\n",
      "Epoch 131: train loss:  0.354, \t train accuracy  0.900 \n",
      "          test loss:  0.288,  \t test accuracy  0.933\n",
      "Epoch 141: train loss:  0.031, \t train accuracy  1.000 \n",
      "          test loss:  0.158,  \t test accuracy  0.967\n",
      "accuracy for resNet is  0.9866666793823242\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:47:39.786209Z",
     "start_time": "2024-06-14T14:47:39.783554Z"
    }
   },
   "source": [
    "# define different background to be used and number of samples as n_background\n",
    "# TODO set a number of TOTAL sampling regardless of the background type?\n",
    "n_background = 50\n",
    "background_types = [\"average\", \"zero\", \"sampling\"] # zero, constant, average, multisample\n",
    "for bt in background_types:\n",
    "\tresults['attributions'][bt] = np.zeros( X_test.shape ,dtype=np.float32 )"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "\t\n",
    "\tSHAP = ShapleyValueSampling(clf) if predictor_name in predictors['torch'] \\\n",
    "\t\telse ShapleyValueSampling(forward_classification)\n",
    "\t\n",
    "\tfor i in trange ( n_samples ) : #\n",
    "\t\t\n",
    "\t\t# get current sample and label\n",
    "\t\tts, y = X_test[i] , torch.tensor( y_test[i:i+1] )\n",
    "\n",
    "\t\t# get segment and its tensor representation\n",
    "\t\tcurrent_segments = segmentation_method(ts)[:X_test.shape[1]]\n",
    "\t\tresults['segments'][i] = current_segments\n",
    "\t\tmask = get_feature_mask(current_segments,ts.shape[-1])\n",
    "\n",
    "\t\tts = torch.tensor(ts).repeat(1,1,1)\t#TODO use something similar to np.expand_dim?\n",
    "\n",
    "\t\tfor background_type in background_types:\n",
    "\n",
    "\t\t\t# background data\n",
    "\t\t\tif background_type==\"zero\":\n",
    "\t\t\t\tbackground_dataset = torch.zeros((1,) + X_train.shape[1:])\n",
    "\t\t\telif background_type==\"sampling\":\n",
    "\t\t\t\tbackground_dataset = sample_background(X_train, n_background)\n",
    "\t\t\telif background_type==\"average\":\n",
    "\t\t\t\tbackground_dataset = sample_background(X_train, n_background).mean(axis=0, keepdim=True)\n",
    "\n",
    "\t\t\t# for sampling strategy repeat the ts many times as the background dataset size\n",
    "\t\t\tts = ts.repeat(background_dataset.shape[0],1,1) if background_type==\"sampling\" else ts\n",
    "\n",
    "\t\t\t# different call depending on predictor type\n",
    "\t\t\tif predictor_name in predictors['scikit']:\n",
    "\t\t\t\t# if using random forest flat everything\n",
    "\t\t\t\tif predictor_name==\"randomForest\":\n",
    "\t\t\t\t\tts = ts.reshape( -1, n_chs*ts_length); mask = mask.reshape( -1, n_chs*ts_length);\n",
    "\t\t\t\t\tbackground_dataset = background_dataset.reshape( -1, n_chs*ts_length)\n",
    "\t\t\t\t\n",
    "\t\t\t\ttmp = SHAP.attribute( ts, target=y , feature_mask=mask, baselines=background_dataset, additional_forward_args=clf)\n",
    "\t\t\t\n",
    "\t\t\telif predictor_name in predictors['torch']:\n",
    "\t\t\t\t# if use torch make sure everything is on selected device\n",
    "\t\t\t\tts = ts.to(device); y = y.to(device)\n",
    "\t\t\t\tmask = mask.to(device) ; background_dataset =  background_dataset.to(device)\n",
    "\t\t\t\ttmp = SHAP.attribute( ts, target=y , feature_mask=mask, baselines=background_dataset)\n",
    "\t\t\t\n",
    "\t\t\t# 'un-flatten' for randomForest\n",
    "\t\t\tif predictor_name==\"randomForest\":\n",
    "\t\t\t\ttmp = tmp.reshape(-1,X_test.shape[1],X_test.shape[2])\n",
    "\n",
    "\t\t\t# store current explanation in the data structure; if sampling store the mean\n",
    "\t\t\tresults['attributions'][background_type][i] = torch.mean(tmp, dim=0).cpu().numpy() if \\\n",
    "\t\t\t\tbackground_type==\"sampling\" else tmp[0].cpu().numpy()\n",
    "\n",
    "\tif segmentation_type==\"normalized\":\n",
    "\t\tweights = np.array(list(map(lambda x: list(map(lambda y: lengths_to_weights(change_points_to_lengths(y, X_train.shape[-1])), x)), results[\"segments\"])))\n",
    "\t\tfor background_type in background_types:\n",
    "\t\t\tresults[\"attributions\"][background_type] *= weights\n",
    "\n",
    "\t\t\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:08:49.634218Z",
     "start_time": "2024-06-11T15:08:49.630046Z"
    }
   },
   "outputs": [],
   "source": [
    "# dump result to disk\n",
    "file_name = \"_\".join ( ( predictor_name, dataset_name ) )+\".npy\"\n",
    "file_path = os.path.join(\"attributions\", file_name)\n",
    "np.save( file_path, results )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_segment_shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
