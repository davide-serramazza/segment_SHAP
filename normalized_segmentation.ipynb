{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:46:18.928809Z",
     "start_time": "2024-06-14T14:46:16.742066Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nikos\\miniconda3\\envs\\env_segment_shap\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from captum.attr import ShapleyValueSampling\n",
    "from tqdm import trange\n",
    "\n",
    "from load_data import load_data\n",
    "from train_models import *\n",
    "from segmentation import *\n",
    "from utils import *\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:47:34.371228Z",
     "start_time": "2024-06-14T14:47:34.368207Z"
    }
   },
   "outputs": [],
   "source": [
    "# device for torch\n",
    "from torch.cuda import is_available as is_GPU_available\n",
    "device = \"cuda\" if is_GPU_available() else \"cpu\"\n",
    "\n",
    "# dictionary mapping predictors to torch vs other, step necessary for Captum \n",
    "predictors = {\n",
    "\t'torch' : ['resNet'],\n",
    "\t'scikit' : ['miniRocket','randomForest']\n",
    "}\n",
    "segmentation_dict = {\"clasp\":get_claSP_segmentation, \"infogain\": get_InformationGain_segmentation, \"greedygaussian\": get_GreedyGaussian_segmentation, \"equal\": get_equal_segmentation, \"nnsegment\": get_NNSegment_segmentation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:47:34.879773Z",
     "start_time": "2024-06-14T14:47:34.876321Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "n_background = 50\n",
    "\n",
    "# settings\n",
    "dataset_names = {'gunpoint'}\n",
    "predictor_names = {\"randomForest\", 'miniRocket', 'resNet'}\n",
    "segmentation_names = {\"clasp\", \"infogain\", \"greedygaussian\", \"equal\", \"nnsegment\"}\n",
    "segmentation_types = {\"default\", \"normalized\"}\n",
    "background_types = {\"average\", \"zero\", \"sampling\"}\n",
    "\n",
    "# demo\n",
    "# dataset_names = {'gunpoint'}\n",
    "# predictor_names = {\"randomForest\"}\n",
    "# segmentation_names = {\"clasp\"}\n",
    "# segmentation_types = {\"default\", \"normalized\"}\n",
    "# background_types = {\"average\", \"zero\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_demo = {\"dataset_name\": \n",
    "                        {\"segmentation_name\": \n",
    "                                        {\"predictor_name\":{\n",
    "                                                'attributions' : {\n",
    "                                                        \"background_name\":{\n",
    "                                                                \"default\": None, \"normalized\": None}\n",
    "                                                },\n",
    "                                                'y_test_pred' : None,        \n",
    "                                        }, \n",
    "                                        \"segments\": None}, \n",
    "                        'y_test_true': None,\n",
    "                        'label_mapping': None}\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training random forest\n",
      "random forest accuracy is 1.0\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    # init dataset\n",
    "    # load data\n",
    "    X_train, X_test, y_train, y_test, enc = load_data(subset='all', dataset_name=dataset_name)\n",
    "    # for debugging only\n",
    "    # X_test = X_test[:2]\n",
    "    # y_test = y_test[:2]\n",
    "    n_samples, n_chs, ts_length = X_test.shape\n",
    "\n",
    "    results[dataset_name] = {'y_test_true': y_test, 'label_mapping':enc}\n",
    "\n",
    "    predictor_dict = dict()\n",
    "    for predictor_name in predictor_names:\n",
    "\n",
    "        if predictor_name=='resNet':\n",
    "            clf,preds = train_ResNet(X_train, y_train, X_test, y_test, dataset_name,device=device)\n",
    "        elif predictor_name=='miniRocket':\n",
    "            clf,preds = train_miniRocket(X_train, y_train, X_test, y_test, dataset_name)\n",
    "        elif predictor_name==\"randomForest\":\n",
    "            clf, preds = train_randomForest(X_train, y_train, X_test, y_test, dataset_name)\n",
    "        else:\n",
    "            raise ValueError(\"predictor not found\")\n",
    "\n",
    "        predictor_dict[predictor_name] = {\"clf\": clf, \"preds\": preds}\n",
    "\n",
    "    for segmentation_name in segmentation_names:\n",
    "\n",
    "        init_segments = np.empty( (X_test.shape[0] , X_test.shape[1]), dtype=object) if X_test.shape[1] > 1  else (np.empty( X_test.shape[0] , dtype=object))\n",
    "        results[dataset_name][segmentation_name] = {\"segments\": init_segments}\n",
    "        for predictor_name in predictor_names:\n",
    "            results[dataset_name][segmentation_name][predictor_name] = {'attributions': dict()}\n",
    "        segmentation_method = segmentation_dict[segmentation_name]\n",
    "\n",
    "        ts_list = []\n",
    "        mask_list = []\n",
    "        y_list = []\n",
    "\n",
    "        for i in range(n_samples) : #\n",
    "            \n",
    "            # get current sample and label\n",
    "            ts, y = X_test[i] , torch.tensor( y_test[i:i+1] )\n",
    "\n",
    "            # get segment and its tensor representation\n",
    "            current_segments = segmentation_method(ts)[:X_test.shape[1]]\n",
    "            results[dataset_name][segmentation_name]['segments'][i] = current_segments\n",
    "            mask = get_feature_mask(current_segments,ts.shape[-1])\n",
    "            mask_list.append(mask)\n",
    "            ts = torch.tensor(ts).repeat(1,1,1)\t#TODO use something similar to np.expand_dim?\n",
    "            ts_list.append(ts)\n",
    "            y_list.append(y)\n",
    "\n",
    "        for background_type in background_types:\n",
    "\n",
    "            # background data\n",
    "            if background_type==\"zero\":\n",
    "                background_dataset = torch.zeros((1,) + X_train.shape[1:])\n",
    "            elif background_type==\"sampling\":\n",
    "                background_dataset = sample_background(X_train, n_background)\n",
    "            elif background_type==\"average\":\n",
    "                background_dataset = sample_background(X_train, n_background).mean(axis=0, keepdim=True)\n",
    "\n",
    "            for predictor_name in predictor_names:\n",
    "\n",
    "                clf = predictor_dict[predictor_name][\"clf\"]\n",
    "                results[dataset_name][segmentation_name][predictor_name]['y_test_pred'] = predictor_dict[predictor_name][\"preds\"]\n",
    "                for background_type in background_types:\n",
    "                    results[dataset_name][segmentation_name][predictor_name]['attributions'][background_type] = {\"default\": np.zeros( X_test.shape ,dtype=np.float32 )}\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    SHAP = ShapleyValueSampling(clf) if predictor_name in predictors['torch'] \\\n",
    "                        else ShapleyValueSampling(forward_classification)\n",
    "\n",
    "                    for i, (ts, mask, y) in enumerate(zip(ts_list, mask_list, y_list)):\n",
    "\n",
    "                        # for sampling strategy repeat the ts many times as the background dataset size\n",
    "                        ts = ts.repeat(background_dataset.shape[0],1,1) if background_type==\"sampling\" else ts\n",
    "                        \n",
    "                        # different call depending on predictor type\n",
    "                        if predictor_name in predictors['scikit']:\n",
    "                            # if using random forest flat everything\n",
    "                            if predictor_name==\"randomForest\":\n",
    "                                ts = ts.reshape( -1, n_chs*ts_length)\n",
    "                                mask = mask.reshape( -1, n_chs*ts_length)\n",
    "                                background_dataset = background_dataset.reshape( -1, n_chs*ts_length)\n",
    "                            \n",
    "                            tmp = SHAP.attribute( ts, target=y , feature_mask=mask, baselines=background_dataset, additional_forward_args=clf)\n",
    "                        \n",
    "                        elif predictor_name in predictors['torch']:\n",
    "                            # if use torch make sure everything is on selected device\n",
    "                            ts = ts.to(device); y = y.to(device)\n",
    "                            mask = mask.to(device) ; background_dataset =  background_dataset.to(device)\n",
    "                            tmp = SHAP.attribute( ts, target=y , feature_mask=mask, baselines=background_dataset)\n",
    "                        \n",
    "                        # 'un-flatten' for randomForest\n",
    "                        if predictor_name==\"randomForest\":\n",
    "                            tmp = tmp.reshape(-1,X_test.shape[1],X_test.shape[2])\n",
    "\n",
    "                        # store current explanation in the data structure; if sampling store the mean\n",
    "                        results[dataset_name][segmentation_name][predictor_name]['attributions'][background_type][\"default\"][i] = torch.mean(tmp, dim=0).cpu().numpy() if \\\n",
    "                            background_type==\"sampling\" else tmp[0].cpu().numpy()\n",
    "\n",
    "                    if \"normalized\" in segmentation_types:\n",
    "                        weights = np.array(list(map(lambda x: list(map(lambda y: lengths_to_weights(change_points_to_lengths(y, X_train.shape[-1])), x)), results[dataset_name][segmentation_name][\"segments\"])))\n",
    "                        for background_type in background_types:\n",
    "                            results[dataset_name][segmentation_name][predictor_name]['attributions'][background_type][\"normalized\"] = results[dataset_name][segmentation_name][predictor_name]['attributions'][background_type][\"default\"] * weights\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'average': {'default': array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0.]],\n",
       "  \n",
       "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0.]]], dtype=float32),\n",
       "  'normalized': array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0.]],\n",
       "  \n",
       "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0.]]])},\n",
       " 'zero': {'default': array([[[ 0.04      ,  0.04      ,  0.04      ,  0.04      ,\n",
       "            0.04      ,  0.04      ,  0.04      ,  0.0468    ,\n",
       "            0.0468    ,  0.0468    ,  0.0468    ,  0.0468    ,\n",
       "            0.0468    ,  0.0468    ,  0.0468    ,  0.0468    ,\n",
       "            0.16439998,  0.16439998,  0.16439998,  0.16439998,\n",
       "            0.16439998,  0.16439998,  0.16439998,  0.16439998,\n",
       "            0.16439998,  0.16439998,  0.16439998,  0.16439998,\n",
       "            0.16439998,  0.16439998,  0.16439998,  0.16439998,\n",
       "            0.16439998,  0.16439998,  0.16439998,  0.16439998,\n",
       "            0.16439998,  0.16439998,  0.16439998,  0.16439998,\n",
       "            0.16439998,  0.16439998,  0.16439998,  0.16439998,\n",
       "            0.16439998,  0.16439998,  0.16439998,  0.31999996,\n",
       "            0.31999996,  0.31999996,  0.31999996,  0.31999996,\n",
       "            0.31999996,  0.31999996,  0.31999996,  0.31999996,\n",
       "            0.31999996,  0.31999996,  0.31999996,  0.31999996,\n",
       "            0.31999996,  0.31999996,  0.31999996,  0.31999996,\n",
       "            0.31999996,  0.31999996,  0.31999996,  0.31999996,\n",
       "            0.31999996,  0.31999996,  0.31999996,  0.31999996,\n",
       "            0.31999996,  0.31999996,  0.31999996,  0.31999996,\n",
       "            0.31999996,  0.31999996,  0.31999996,  0.31999996,\n",
       "            0.31999996,  0.31999996,  0.31999996,  0.31999996,\n",
       "            0.31999996,  0.31999996,  0.31999996,  0.31999996,\n",
       "            0.31999996,  0.31999996,  0.31999996,  0.31999996,\n",
       "            0.31999996,  0.31999996,  0.31999996,  0.31999996,\n",
       "            0.02719999,  0.02719999,  0.02719999,  0.02719999,\n",
       "            0.02719999,  0.02719999,  0.02719999,  0.02719999,\n",
       "            0.02719999,  0.02719999,  0.02719999,  0.02719999,\n",
       "            0.02719999, -0.0052    , -0.0052    , -0.0052    ,\n",
       "           -0.0052    , -0.0052    , -0.0052    , -0.0052    ,\n",
       "           -0.0052    , -0.0052    ,  0.1368    ,  0.1368    ,\n",
       "            0.1368    ,  0.1368    ,  0.1368    ,  0.1368    ,\n",
       "            0.1368    ,  0.1368    ,  0.1368    ,  0.1368    ,\n",
       "            0.1368    ,  0.1368    ,  0.1368    ,  0.1368    ,\n",
       "            0.1368    ,  0.1368    ,  0.1368    ,  0.1368    ,\n",
       "            0.1368    ,  0.1368    ,  0.1368    ,  0.1368    ,\n",
       "            0.1368    ,  0.1368    ,  0.1368    ,  0.1368    ,\n",
       "            0.1368    ,  0.1368    ,  0.1368    ,  0.1368    ,\n",
       "            0.1368    ,  0.1368    ]],\n",
       "  \n",
       "         [[-0.07880001, -0.07880001, -0.07880001, -0.07880001,\n",
       "           -0.07880001, -0.07880001, -0.07880001, -0.07880001,\n",
       "           -0.07880001, -0.07880001, -0.07880001, -0.07880001,\n",
       "           -0.07880001, -0.07880001, -0.07880001, -0.07880001,\n",
       "           -0.07880001, -0.07880001, -0.07880001, -0.07880001,\n",
       "           -0.07880001, -0.07880001, -0.07880001, -0.07880001,\n",
       "           -0.07880001, -0.07880001, -0.07880001, -0.07880001,\n",
       "           -0.07880001, -0.07880001, -0.07880001, -0.07880001,\n",
       "           -0.07880001, -0.07880001, -0.07880001,  0.0068    ,\n",
       "            0.0068    ,  0.0068    ,  0.0068    ,  0.0068    ,\n",
       "            0.0068    ,  0.0068    ,  0.0068    ,  0.0068    ,\n",
       "            0.0068    , -0.06119999, -0.06119999, -0.06119999,\n",
       "           -0.06119999, -0.06119999, -0.06119999, -0.06119999,\n",
       "           -0.06119999, -0.06119999, -0.06119999, -0.06119999,\n",
       "           -0.06119999, -0.06119999, -0.06119999, -0.06119999,\n",
       "           -0.06119999, -0.06119999, -0.06119999, -0.06119999,\n",
       "           -0.06119999, -0.06119999, -0.06119999, -0.06119999,\n",
       "           -0.0292    , -0.0292    , -0.0292    , -0.0292    ,\n",
       "           -0.0292    , -0.0292    , -0.0292    , -0.0292    ,\n",
       "           -0.0292    , -0.0292    , -0.0324    , -0.0324    ,\n",
       "           -0.0324    , -0.0324    , -0.0324    , -0.0324    ,\n",
       "           -0.0324    , -0.0324    , -0.0324    , -0.0324    ,\n",
       "           -0.0324    , -0.0324    , -0.0324    , -0.0324    ,\n",
       "           -0.0324    , -0.0324    , -0.0324    , -0.0324    ,\n",
       "           -0.0324    , -0.0324    , -0.0324    , -0.0324    ,\n",
       "           -0.0324    ,  0.07319999,  0.07319999,  0.07319999,\n",
       "            0.07319999,  0.07319999,  0.07319999,  0.07319999,\n",
       "            0.07319999,  0.07319999,  0.07319999,  0.07319999,\n",
       "            0.07319999,  0.07319999,  0.07319999,  0.07319999,\n",
       "            0.07319999,  0.07319999,  0.07319999,  0.07319999,\n",
       "            0.07319999,  0.07319999,  0.07319999,  0.07319999,\n",
       "            0.07319999,  0.07319999,  0.07319999,  0.07319999,\n",
       "            0.07319999,  0.07319999,  0.07319999,  0.07319999,\n",
       "           -0.0084    , -0.0084    , -0.0084    , -0.0084    ,\n",
       "           -0.0084    , -0.0084    , -0.0084    , -0.0084    ,\n",
       "           -0.0084    , -0.0084    , -0.0084    , -0.0084    ,\n",
       "           -0.0084    , -0.0084    , -0.0084    , -0.0084    ,\n",
       "           -0.0084    , -0.0084    ]]], dtype=float32),\n",
       "  'normalized': array([[[ 0.00571429,  0.00571429,  0.00571429,  0.00571429,\n",
       "            0.00571429,  0.00571429,  0.00571429,  0.0052    ,\n",
       "            0.0052    ,  0.0052    ,  0.0052    ,  0.0052    ,\n",
       "            0.0052    ,  0.0052    ,  0.0052    ,  0.0052    ,\n",
       "            0.00530323,  0.00530323,  0.00530323,  0.00530323,\n",
       "            0.00530323,  0.00530323,  0.00530323,  0.00530323,\n",
       "            0.00530323,  0.00530323,  0.00530323,  0.00530323,\n",
       "            0.00530323,  0.00530323,  0.00530323,  0.00530323,\n",
       "            0.00530323,  0.00530323,  0.00530323,  0.00530323,\n",
       "            0.00530323,  0.00530323,  0.00530323,  0.00530323,\n",
       "            0.00530323,  0.00530323,  0.00530323,  0.00530323,\n",
       "            0.00530323,  0.00530323,  0.00530323,  0.00653061,\n",
       "            0.00653061,  0.00653061,  0.00653061,  0.00653061,\n",
       "            0.00653061,  0.00653061,  0.00653061,  0.00653061,\n",
       "            0.00653061,  0.00653061,  0.00653061,  0.00653061,\n",
       "            0.00653061,  0.00653061,  0.00653061,  0.00653061,\n",
       "            0.00653061,  0.00653061,  0.00653061,  0.00653061,\n",
       "            0.00653061,  0.00653061,  0.00653061,  0.00653061,\n",
       "            0.00653061,  0.00653061,  0.00653061,  0.00653061,\n",
       "            0.00653061,  0.00653061,  0.00653061,  0.00653061,\n",
       "            0.00653061,  0.00653061,  0.00653061,  0.00653061,\n",
       "            0.00653061,  0.00653061,  0.00653061,  0.00653061,\n",
       "            0.00653061,  0.00653061,  0.00653061,  0.00653061,\n",
       "            0.00653061,  0.00653061,  0.00653061,  0.00653061,\n",
       "            0.00209231,  0.00209231,  0.00209231,  0.00209231,\n",
       "            0.00209231,  0.00209231,  0.00209231,  0.00209231,\n",
       "            0.00209231,  0.00209231,  0.00209231,  0.00209231,\n",
       "            0.00209231, -0.00057778, -0.00057778, -0.00057778,\n",
       "           -0.00057778, -0.00057778, -0.00057778, -0.00057778,\n",
       "           -0.00057778, -0.00057778,  0.004275  ,  0.004275  ,\n",
       "            0.004275  ,  0.004275  ,  0.004275  ,  0.004275  ,\n",
       "            0.004275  ,  0.004275  ,  0.004275  ,  0.004275  ,\n",
       "            0.004275  ,  0.004275  ,  0.004275  ,  0.004275  ,\n",
       "            0.004275  ,  0.004275  ,  0.004275  ,  0.004275  ,\n",
       "            0.004275  ,  0.004275  ,  0.004275  ,  0.004275  ,\n",
       "            0.004275  ,  0.004275  ,  0.004275  ,  0.004275  ,\n",
       "            0.004275  ,  0.004275  ,  0.004275  ,  0.004275  ,\n",
       "            0.004275  ,  0.004275  ]],\n",
       "  \n",
       "         [[-0.00225143, -0.00225143, -0.00225143, -0.00225143,\n",
       "           -0.00225143, -0.00225143, -0.00225143, -0.00225143,\n",
       "           -0.00225143, -0.00225143, -0.00225143, -0.00225143,\n",
       "           -0.00225143, -0.00225143, -0.00225143, -0.00225143,\n",
       "           -0.00225143, -0.00225143, -0.00225143, -0.00225143,\n",
       "           -0.00225143, -0.00225143, -0.00225143, -0.00225143,\n",
       "           -0.00225143, -0.00225143, -0.00225143, -0.00225143,\n",
       "           -0.00225143, -0.00225143, -0.00225143, -0.00225143,\n",
       "           -0.00225143, -0.00225143, -0.00225143,  0.00068   ,\n",
       "            0.00068   ,  0.00068   ,  0.00068   ,  0.00068   ,\n",
       "            0.00068   ,  0.00068   ,  0.00068   ,  0.00068   ,\n",
       "            0.00068   , -0.00266087, -0.00266087, -0.00266087,\n",
       "           -0.00266087, -0.00266087, -0.00266087, -0.00266087,\n",
       "           -0.00266087, -0.00266087, -0.00266087, -0.00266087,\n",
       "           -0.00266087, -0.00266087, -0.00266087, -0.00266087,\n",
       "           -0.00266087, -0.00266087, -0.00266087, -0.00266087,\n",
       "           -0.00266087, -0.00266087, -0.00266087, -0.00266087,\n",
       "           -0.00292   , -0.00292   , -0.00292   , -0.00292   ,\n",
       "           -0.00292   , -0.00292   , -0.00292   , -0.00292   ,\n",
       "           -0.00292   , -0.00292   , -0.0014087 , -0.0014087 ,\n",
       "           -0.0014087 , -0.0014087 , -0.0014087 , -0.0014087 ,\n",
       "           -0.0014087 , -0.0014087 , -0.0014087 , -0.0014087 ,\n",
       "           -0.0014087 , -0.0014087 , -0.0014087 , -0.0014087 ,\n",
       "           -0.0014087 , -0.0014087 , -0.0014087 , -0.0014087 ,\n",
       "           -0.0014087 , -0.0014087 , -0.0014087 , -0.0014087 ,\n",
       "           -0.0014087 ,  0.00236129,  0.00236129,  0.00236129,\n",
       "            0.00236129,  0.00236129,  0.00236129,  0.00236129,\n",
       "            0.00236129,  0.00236129,  0.00236129,  0.00236129,\n",
       "            0.00236129,  0.00236129,  0.00236129,  0.00236129,\n",
       "            0.00236129,  0.00236129,  0.00236129,  0.00236129,\n",
       "            0.00236129,  0.00236129,  0.00236129,  0.00236129,\n",
       "            0.00236129,  0.00236129,  0.00236129,  0.00236129,\n",
       "            0.00236129,  0.00236129,  0.00236129,  0.00236129,\n",
       "           -0.00046667, -0.00046667, -0.00046667, -0.00046667,\n",
       "           -0.00046667, -0.00046667, -0.00046667, -0.00046667,\n",
       "           -0.00046667, -0.00046667, -0.00046667, -0.00046667,\n",
       "           -0.00046667, -0.00046667, -0.00046667, -0.00046667,\n",
       "           -0.00046667, -0.00046667]]])}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump result to disk\n",
    "file_name = \"all_results\" # \"_\".join ( ( predictor_name, dataset_name ) )+\".npy\"\n",
    "file_path = os.path.join(\"attributions\", file_name)\n",
    "np.save( file_path, results )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD keep temporarily -  to be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:47:39.782779Z",
     "start_time": "2024-06-14T14:47:35.430030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ResNet\n",
      "Epoch 1: train loss:  0.699, \t train accuracy  0.440 \n",
      "          test loss:  0.694,  \t test accuracy  0.493\n",
      "Epoch 11: train loss:  0.688, \t train accuracy  0.520 \n",
      "          test loss:  0.691,  \t test accuracy  0.627\n",
      "Epoch 21: train loss:  0.628, \t train accuracy  0.640 \n",
      "          test loss:  0.627,  \t test accuracy  0.680\n",
      "Epoch 31: train loss:  0.518, \t train accuracy  0.780 \n",
      "          test loss:  0.588,  \t test accuracy  0.700\n",
      "Epoch 41: train loss:  0.447, \t train accuracy  0.800 \n",
      "          test loss:  0.561,  \t test accuracy  0.707\n",
      "Epoch 51: train loss:  0.489, \t train accuracy  0.800 \n",
      "          test loss:  0.568,  \t test accuracy  0.713\n",
      "Epoch 61: train loss:  0.444, \t train accuracy  0.800 \n",
      "          test loss:  0.551,  \t test accuracy  0.720\n",
      "Epoch 71: train loss:  0.442, \t train accuracy  0.760 \n",
      "          test loss:  0.551,  \t test accuracy  0.720\n",
      "Epoch 81: train loss:  0.391, \t train accuracy  0.820 \n",
      "          test loss:  0.521,  \t test accuracy  0.767\n",
      "Epoch 91: train loss:  0.480, \t train accuracy  0.780 \n",
      "          test loss:  0.544,  \t test accuracy  0.780\n",
      "Epoch 101: train loss:  0.431, \t train accuracy  0.780 \n",
      "          test loss:  0.544,  \t test accuracy  0.780\n",
      "Epoch 111: train loss:  0.403, \t train accuracy  0.800 \n",
      "          test loss:  0.544,  \t test accuracy  0.780\n",
      "Epoch 121: train loss:  0.330, \t train accuracy  0.900 \n",
      "          test loss:  0.438,  \t test accuracy  0.887\n",
      "Epoch 131: train loss:  0.354, \t train accuracy  0.900 \n",
      "          test loss:  0.288,  \t test accuracy  0.933\n",
      "Epoch 141: train loss:  0.031, \t train accuracy  1.000 \n",
      "          test loss:  0.158,  \t test accuracy  0.967\n",
      "accuracy for resNet is  0.9866666793823242\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # train model\n",
    "# predictor_name = 'resNet'\n",
    "# if predictor_name=='resNet':\n",
    "# \tclf,preds = train_ResNet(X_train, y_train, X_test, y_test, dataset_name,device=device)\n",
    "# elif predictor_name=='miniRocket':\n",
    "# \tclf,preds = train_miniRocket(X_train, y_train, X_test, y_test, dataset_name)\n",
    "# elif predictor_name==\"randomForest\":\n",
    "# \tclf, preds = train_randomForest(X_train, y_train, X_test, y_test, dataset_name)\n",
    "# else:\n",
    "# \traise ValueError(\n",
    "# \t\t\"predictor not found\"\n",
    "# \t)\n",
    "\n",
    "# # segmentation\n",
    "# segmentation_name = \"nnsegment\"\n",
    "# segmentation_type = \"normalized\"\n",
    "# segmentation_method = segmentation_dict[segmentation_name]\n",
    "\n",
    "# # initialize data structure meant to contain the segments\n",
    "# # TODO can I be cleaner here?\n",
    "# segments =  np.empty( (X_test.shape[0] , X_test.shape[1]), dtype=object) if X_test.shape[1] > 1  else (\n",
    "# \tnp.empty( X_test.shape[0] , dtype=object))\n",
    "\n",
    "# # create a dictionary to be dumped containing attribution and metadata\n",
    "# results = {\n",
    "# \t'attributions' : {},\n",
    "# \t'segments' : segments,\n",
    "# \t'y_test_true' : y_test,\n",
    "# \t'y_test_pred' : preds,\n",
    "# \t'label_mapping' : enc,\n",
    "# }\n",
    "\n",
    "# # result # dataset # model# segment # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:47:39.786209Z",
     "start_time": "2024-06-14T14:47:39.783554Z"
    }
   },
   "outputs": [],
   "source": [
    "# # define different background to be used and number of samples as n_background\n",
    "# # TODO set a number of TOTAL sampling regardless of the background type?\n",
    "# # zero, constant, average, multisample\n",
    "# for bt in background_types:\n",
    "# \tresults['attributions'][bt] = np.zeros( X_test.shape ,dtype=np.float32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "\t\n",
    "# \tSHAP = ShapleyValueSampling(clf) if predictor_name in predictors['torch'] \\\n",
    "# \t\telse ShapleyValueSampling(forward_classification)\n",
    "\t\n",
    "# \tfor i in trange ( n_samples ) : #\n",
    "\t\t\n",
    "# \t\t# get current sample and label\n",
    "# \t\tts, y = X_test[i] , torch.tensor( y_test[i:i+1] )\n",
    "\n",
    "# \t\t# get segment and its tensor representation\n",
    "# \t\tcurrent_segments = segmentation_method(ts)[:X_test.shape[1]]\n",
    "# \t\tresults['segments'][i] = current_segments\n",
    "# \t\tmask = get_feature_mask(current_segments,ts.shape[-1])\n",
    "\n",
    "# \t\tts = torch.tensor(ts).repeat(1,1,1)\t#TODO use something similar to np.expand_dim?\n",
    "\n",
    "# \t\tfor background_type in background_types:\n",
    "\n",
    "# \t\t\t# background data\n",
    "# \t\t\tif background_type==\"zero\":\n",
    "# \t\t\t\tbackground_dataset = torch.zeros((1,) + X_train.shape[1:])\n",
    "# \t\t\telif background_type==\"sampling\":\n",
    "# \t\t\t\tbackground_dataset = sample_background(X_train, n_background)\n",
    "# \t\t\telif background_type==\"average\":\n",
    "# \t\t\t\tbackground_dataset = sample_background(X_train, n_background).mean(axis=0, keepdim=True)\n",
    "\n",
    "# \t\t\t# for sampling strategy repeat the ts many times as the background dataset size\n",
    "# \t\t\tts = ts.repeat(background_dataset.shape[0],1,1) if background_type==\"sampling\" else ts\n",
    "\n",
    "# \t\t\t# different call depending on predictor type\n",
    "# \t\t\tif predictor_name in predictors['scikit']:\n",
    "# \t\t\t\t# if using random forest flat everything\n",
    "# \t\t\t\tif predictor_name==\"randomForest\":\n",
    "# \t\t\t\t\tts = ts.reshape( -1, n_chs*ts_length); mask = mask.reshape( -1, n_chs*ts_length);\n",
    "# \t\t\t\t\tbackground_dataset = background_dataset.reshape( -1, n_chs*ts_length)\n",
    "\t\t\t\t\n",
    "# \t\t\t\ttmp = SHAP.attribute( ts, target=y , feature_mask=mask, baselines=background_dataset, additional_forward_args=clf)\n",
    "\t\t\t\n",
    "# \t\t\telif predictor_name in predictors['torch']:\n",
    "# \t\t\t\t# if use torch make sure everything is on selected device\n",
    "# \t\t\t\tts = ts.to(device); y = y.to(device)\n",
    "# \t\t\t\tmask = mask.to(device) ; background_dataset =  background_dataset.to(device)\n",
    "# \t\t\t\ttmp = SHAP.attribute( ts, target=y , feature_mask=mask, baselines=background_dataset)\n",
    "\t\t\t\n",
    "# \t\t\t# 'un-flatten' for randomForest\n",
    "# \t\t\tif predictor_name==\"randomForest\":\n",
    "# \t\t\t\ttmp = tmp.reshape(-1,X_test.shape[1],X_test.shape[2])\n",
    "\n",
    "# \t\t\t# store current explanation in the data structure; if sampling store the mean\n",
    "# \t\t\tresults['attributions'][background_type][i] = torch.mean(tmp, dim=0).cpu().numpy() if \\\n",
    "# \t\t\t\tbackground_type==\"sampling\" else tmp[0].cpu().numpy()\n",
    "\n",
    "# \tif segmentation_type==\"normalized\":\n",
    "# \t\tweights = np.array(list(map(lambda x: list(map(lambda y: lengths_to_weights(change_points_to_lengths(y, X_train.shape[-1])), x)), results[\"segments\"])))\n",
    "# \t\tfor background_type in background_types:\n",
    "# \t\t\tresults[\"attributions\"][background_type] *= weights\n",
    "\n",
    "\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T15:08:49.634218Z",
     "start_time": "2024-06-11T15:08:49.630046Z"
    }
   },
   "outputs": [],
   "source": [
    "# # dump result to disk\n",
    "# file_name = \"_\".join ( ( predictor_name, dataset_name ) )+\".npy\"\n",
    "# file_path = os.path.join(\"attributions\", file_name)\n",
    "# np.save( file_path, results )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_segment_shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
