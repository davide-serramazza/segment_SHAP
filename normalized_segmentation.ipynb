{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:18:10.477821Z",
     "start_time": "2024-08-23T10:18:10.322155Z"
    }
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:18:53.721301Z",
     "start_time": "2024-08-23T10:18:53.718743Z"
    }
   },
   "outputs": [],
   "source": [
    "from captum.attr import ShapleyValueSampling\n",
    "from load_data import load_data\n",
    "from models.model_wrappers import *\n",
    "from models.train_models import *\n",
    "from segmentation import *\n",
    "from utils import *\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import timeit\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# device for torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:18:54.656565Z",
     "start_time": "2024-08-23T10:18:54.652262Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.cuda import is_available as is_GPU_available\n",
    "device = \"cuda\" if is_GPU_available() else \"cpu\"\n",
    "\n",
    "# dictionary mapping predictors to torch vs other, step necessary for Captum \n",
    "predictors = {\n",
    "\t'torch' : ['resNet'],\n",
    "\t'scikit' : ['miniRocket','randomForest','QUANT']\n",
    "}\n",
    "segmentation_dict = {\"clasp\":get_claSP_segmentation, \"infogain\": get_InformationGain_segmentation, \"greedygaussian\": get_GreedyGaussian_segmentation, \"equal\": get_equal_segmentation, \"nnsegment\": get_NNSegment_segmentation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:18:56.577079Z",
     "start_time": "2024-08-23T10:18:56.573443Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "n_background = 50\n",
    "\n",
    "# settings\n",
    "dataset_names = {'gunpoint'}    #{sys.argv[1]}\n",
    "predictor_names = {'miniRocket'}    #{sys.argv[2]} {\"randomForest\", 'miniRocket', 'resNet'}\n",
    "segmentation_names = {\"clasp\",\"greedygaussian\", \"equal\", \"infogain\",\"nnsegment\"} # {\"clasp\",\"greedygaussian\", \"equal\", \"infogain\",\"nnsegment\"} \n",
    "background_names =  {\"average\", \"zero\",\"sampling\"} #{\"average\", \"zero\", \"sampling\"}\n",
    "normalization_names = {\"default\", \"normalized\"}\n",
    "\n",
    "demo_mode = True\n",
    "# demo\n",
    "if demo_mode:\n",
    "    dataset_names = {'gunpoint'}\n",
    "    predictor_names = {\"QUANT\"}\n",
    "    segmentation_names = { \"equal\",'clasp'}\n",
    "    background_names ={\"average\",\"sampling\"} #,'sampling'}\n",
    "    normalization_names = {\"default\", \"normalized\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# instantiate the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:18:57.177479Z",
     "start_time": "2024-08-23T10:18:57.174927Z"
    }
   },
   "outputs": [],
   "source": [
    "results = dict.fromkeys(('y_test_true', 'label_mapping', \"segments\", 'y_test_pred', \"attributions\"))\n",
    "for key in results.keys():\n",
    "    results[key] = dict.fromkeys(dataset_names)\n",
    "normalization_names = normalization_names | {\"default\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:18:57.857757Z",
     "start_time": "2024-08-23T10:18:57.671934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training QUANT\n",
      "accuracy for QUANT is  1.0\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in dataset_names:\n",
    "    # init dataset\n",
    "    # load data\n",
    "    X_train, X_test, y_train, y_test, enc = load_data(subset='all', dataset_name=dataset_name)\n",
    "    # for debugging only\n",
    "    if demo_mode:\n",
    "        X_test = X_test[:2]\n",
    "        y_test = y_test[:2]\n",
    "\n",
    "    n_samples, n_chs, ts_length = X_test.shape\n",
    "\n",
    "    results['y_test_true'][dataset_name] = y_test\n",
    "    results['label_mapping'][dataset_name] = enc\n",
    "    results[\"attributions\"][dataset_name] = dict.fromkeys(segmentation_names)\n",
    "    results[\"segments\"][dataset_name] = dict.fromkeys(segmentation_names)\n",
    "    results[\"y_test_pred\"][dataset_name] = dict.fromkeys(predictor_names)\n",
    "\n",
    "    predictor_dict = dict()\n",
    "    # TODO not to save if in demo mode!\n",
    "    for predictor_name in predictor_names:\n",
    "        if demo_mode:\n",
    "            dataset_name=None\n",
    "\n",
    "        if predictor_name=='resNet':\n",
    "            clf,preds = train_ResNet(X_train, y_train, X_test, y_test, dataset_name,device=device)\n",
    "        elif predictor_name=='miniRocket':\n",
    "            clf,preds = train_miniRocket(X_train, y_train, X_test, y_test, dataset_name)\n",
    "        elif predictor_name==\"randomForest\":\n",
    "            clf, preds = train_randomForest(X_train, y_train, X_test, y_test, dataset_name)\n",
    "        elif predictor_name==\"QUANT\":\n",
    "            clf, preds = train_QUANT(X_train, y_train, X_test, y_test, dataset_name)\n",
    "        else:\n",
    "            raise ValueError(\"predictor not found\")\n",
    "\n",
    "        predictor_dict[predictor_name] = {\"clf\": clf, \"preds\": preds}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:18:57.968835Z",
     "start_time": "2024-08-23T10:18:57.961771Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_result_dict(X_test,predictor_names,dataset_name,segmentation_name,results):\n",
    "\tinit_segments = np.empty((X_test.shape[0], X_test.shape[1]), dtype=object) if X_test.shape[1] > 1 else (\n",
    "\t\tnp.empty(X_test.shape[0], dtype=object))\n",
    "\tresults[\"segments\"][dataset_name][segmentation_name] = init_segments.copy()\n",
    "\tresults[\"attributions\"][dataset_name][segmentation_name] = dict.fromkeys(predictor_names)\n",
    "\tfor predictor_name in predictor_names:\n",
    "\t\tresults[\"attributions\"][dataset_name][segmentation_name][predictor_name] = dict.fromkeys(background_names)\n",
    "\n",
    "\n",
    "def get_sample_info(X_test,y_test,results, id):\n",
    "\t# get current sample and label\n",
    "\tts, y = X_test[id], torch.tensor(y_test[id:id + 1])\n",
    "\t# get segment and its tensor representation\n",
    "\tcurrent_segments = segmentation_method(ts)[:X_test.shape[1]]\n",
    "\tresults['segments'][dataset_name][segmentation_name][i] = current_segments\n",
    "\tmask = get_feature_mask(current_segments, ts.shape[-1])\n",
    "\tmask_list.append(mask)\n",
    "\tts = torch.tensor(ts).repeat(1, 1, 1)  # TODO use something similar to np.expand_dim?\n",
    "\tts_list.append(ts)\n",
    "\ty_list.append(y)\n",
    "\treturn ts,y,mask\n",
    "\n",
    "\n",
    "def get_background( background_name, results, normalization_names):\n",
    "\n",
    "\tresults[\"attributions\"][dataset_name][segmentation_name][predictor_name][background_name] = dict.fromkeys(\n",
    "\t\tnormalization_names)\n",
    "\t# background data\n",
    "\tif background_name == \"zero\":\n",
    "\t\tbackground_dataset = torch.zeros((1,) + X_train.shape[1:])\n",
    "\telif background_name == \"sampling\":\n",
    "\t\tbackground_dataset = sample_background(X_train, n_background)\n",
    "\telif background_name == \"average\":\n",
    "\t\tbackground_dataset = sample_background(X_train, n_background).mean(axis=0, keepdim=True)\n",
    "\n",
    "\treturn background_dataset\n",
    "\n",
    "\n",
    "def get_attribution(ts, mask, background_dataset,y, results ): #    global ts, mask, background_dataset, tmp, y\n",
    "\t# different call depending on predictor type\n",
    "\tif predictor_name in predictors['scikit']:\n",
    "\t\t# if using random forest flat everything\n",
    "\t\tif predictor_name == \"randomForest\":\n",
    "\t\t\tts = ts.reshape(-1, n_chs * ts_length)\n",
    "\t\t\tmask = mask.reshape(-1, n_chs * ts_length)\n",
    "\t\t\tbackground_dataset = background_dataset.reshape(-1, n_chs * ts_length)\n",
    "\n",
    "\t\ttmp = SHAP.attribute(ts, target=y, feature_mask=mask, baselines=background_dataset, additional_forward_args=clf)\n",
    "\n",
    "\telif predictor_name in predictors['torch']:\n",
    "\t\t# if use torch make sure everything is on selected device\n",
    "\t\tts = ts.to(device);\n",
    "\t\ty = y.to(device)\n",
    "\t\tmask = mask.to(device);\n",
    "\t\tbackground_dataset = background_dataset.to(device)\n",
    "\t\ttmp = SHAP.attribute(ts, target=y, feature_mask=mask, baselines=background_dataset)\n",
    "\n",
    "\t# in case of random forest 'un-flatten' result\n",
    "\tif predictor_name==\"randomForest\":\n",
    "\t\ttmp = tmp.reshape(-1,X_test.shape[1],X_test.shape[2])\n",
    "\n",
    "\t# lastly store current explanation in the data structure; if sampling store the mean\n",
    "\tresults['attributions'][dataset_name][segmentation_name][predictor_name][background_name][\"default\"][i] = torch.mean(tmp, dim=0).cpu().numpy() if \\\n",
    "\t\tbackground_name==\"sampling\" else tmp[0].cpu().numpy()\n",
    "\n",
    "\n",
    "def get_normalized_results(normalization_names,results):\n",
    "\tif \"normalized\" in normalization_names:\n",
    "\t\tweights = np.array(list(map(\n",
    "\t\t\tlambda segmentation: list(map(\n",
    "\t\t\t\tlambda channel_segemnts: lengths_to_weights(change_points_to_lengths(channel_segments, X_train.shape[-1])),\n",
    "\t\t\t\tsegmentation)),\n",
    "\t\t\tresults[\"segments\"][dataset_name][segmentation_name])))\n",
    "\n",
    "\t\tresults['attributions'][dataset_name][segmentation_name][predictor_name][background_name][\"normalized\"] = \\\n",
    "\t\t\tresults['attributions'][dataset_name][segmentation_name][predictor_name][background_name][\"default\"] * weights\n",
    "\tif \"default\" not in normalization_names:\n",
    "\t\tdel results['attributions'][dataset_name][segmentation_name][predictor_name][background_name][\"default\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:19:34.077004Z",
     "start_time": "2024-08-23T10:18:59.001328Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.25s/it]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.99s/it]\n",
      "100%|██████████| 2/2 [00:08<00:00,  4.45s/it]\n",
      "100%|██████████| 2/2 [00:11<00:00,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time 35.07191439400049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "starttime = timeit.default_timer()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset_name in dataset_names:\n",
    "        for predictor_name in predictor_names:\n",
    "            results['y_test_pred'][dataset_name][predictor_name] = predictor_dict[predictor_name][\"preds\"]\n",
    "        for segmentation_name in segmentation_names:\n",
    "            initialize_result_dict(X_test,predictor_names,dataset_name,segmentation_name,results)\n",
    "            segmentation_method = segmentation_dict[segmentation_name]\n",
    "            \n",
    "            ts_list = []\n",
    "            mask_list = []\n",
    "            y_list = []\n",
    "            \n",
    "            for i in range(n_samples) : \n",
    "                ts,y,mask = get_sample_info(X_test,y_test, results, i)\n",
    "            \n",
    "            for background_name in background_names:\n",
    "                background_dataset = get_background( background_name, results, normalization_names)\n",
    "                for predictor_name in predictor_names:\n",
    "                    # get clf and initialize attributions\n",
    "                    clf = predictor_dict[predictor_name][\"clf\"]\n",
    "                    init_attributions = np.zeros(X_test.shape, dtype=np.float32)\n",
    "                    for normalization_name in normalization_names:\n",
    "                        results['attributions'][dataset_name][segmentation_name][predictor_name][background_name][normalization_name] = init_attributions.copy()\n",
    "\n",
    "                    \n",
    "                    SHAP = ShapleyValueSampling(clf) if predictor_name in predictors['torch'] else ShapleyValueSampling(forward_classification)\n",
    "                    \n",
    "                    with tqdm(total=len(ts_list)) as pbar:\n",
    "                        for i, (ts, mask, y) in enumerate(zip(ts_list, mask_list, y_list)):\n",
    "                            # for sampling strategy repeat the ts many times as the background dataset size\n",
    "                            ts = ts.repeat(background_dataset.shape[0],1,1) if background_name==\"sampling\" else ts\n",
    "                            get_attribution (ts, mask, background_dataset,y, results )\n",
    "                            # update tqdm\n",
    "                            pbar.update(1)\n",
    "                            \n",
    "                    pbar.close()\n",
    "                    get_normalized_results(normalization_names,results)\n",
    "                    \n",
    "print(\"elapsed time\", ( timeit.default_timer() -starttime ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T13:31:12.579955Z",
     "start_time": "2024-08-22T13:31:12.569686Z"
    }
   },
   "outputs": [],
   "source": [
    "# dump result to disk\n",
    "if not demo_mode:\n",
    "    file_name = \"_\".join( (\"all_results\",dataset_name,predictor_name) )\n",
    "else:\n",
    "\tfile_name = \"_\".join( (\"all_results_DEMO_\",dataset_name,predictor_name) )\n",
    "file_path = os.path.join(\"attributions\", file_name)\n",
    "np.save( file_path, results )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-23T10:19:34.089808Z",
     "start_time": "2024-08-23T10:19:34.086307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gunpoint': {'QUANT': array([[0.985, 0.015],\n",
       "         [0.015, 0.985]])}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"y_test_pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "scipy.stats.entropy([0.5,], base=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_segment_shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
