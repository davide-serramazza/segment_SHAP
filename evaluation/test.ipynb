{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "#import torch.cuda\n",
    "import timeit\n",
    "import pandas as pd\n",
    "#import argparse\n",
    "import itertools\n",
    "from segment_SHAP.load_data import load_data # segment_SHAP\n",
    "#from pickle import dump\n",
    "\n",
    "# metric specific\n",
    "from sklearn.metrics import auc as sklearn_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "dataset_names = {'MP'}    #{sys.argv[1]}\n",
    "predictor_names = {'randomForest'}    #{sys.argv[2]} {\"randomForest\", 'miniRocket', 'resNet'}\n",
    "segmentation_names = {\"clasp\",\"greedygaussian\", \"equal\", \"infogain\",\"nnsegment\"} # {\"clasp\",\"greedygaussian\", \"equal\", \"infogain\",\"nnsegment\"} \n",
    "background_names =  {\"average\", \"zero\",\"sampling\"} #{\"average\", \"zero\", \"sampling\"}\n",
    "normalization_names = {\"default\", \"normalized\"}\n",
    "\n",
    "metric_names = {\"dummy\"}\n",
    "\n",
    "demo_mode = True\n",
    "# demo\n",
    "if demo_mode:\n",
    "    dataset_names = {'gunpoint'}\n",
    "    predictor_names = {'resNet'}\n",
    "    segmentation_names = {'clasp'}\n",
    "    background_names = {'zero'} #,'sampling'}\n",
    "    normalization_names = {\"default\", \"normalized\"}\n",
    "\n",
    "# optional\n",
    "# # get infos about which explanations are evaluated\n",
    "# datasets = list( explanations['attributions'].keys() )\n",
    "# segmentations = list( explanations['attributions'][datasets[0]].keys() )\n",
    "# predictors = list( explanations['attributions'][datasets[0]][segmentations[0]].keys() )\n",
    "# backgrounds = list( explanations['attributions'][datasets[0]][segmentations[0]][predictors[0]].keys() )\n",
    "# result_types = ['default','normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "datasets_folder_path = None\n",
    "attributions_folder_path = None\n",
    "trained_models_folder_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  gunpoint\n",
      "Predictor:  resNet\n",
      "assessing ('clasp', 'zero', 'normalized', 'dummy')\n",
      "elapsed time 0.05957539996597916\n",
      "assessing ('clasp', 'zero', 'default', 'dummy')\n",
      "elapsed time 0.05960319994483143\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "if datasets_folder_path is None:\n",
    "    datasets_folder_path = os.path.join(os.path.dirname(cwd), \"datasets\")\n",
    "if attributions_folder_path is None:\n",
    "    attributions_folder_path = os.path.join(os.path.dirname(cwd), \"attributions\")\n",
    "if trained_models_folder_path is None:\n",
    "    trained_models_folder_path = os.path.join(os.path.dirname(cwd), \"trained_models\")\n",
    "\n",
    "# device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "starttime = timeit.default_timer()\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(\"Dataset: \", dataset_name)\n",
    "    # loading dataset\n",
    "    X_train, X_test, y_train, y_test, enc = load_data(subset='all', dataset_name=dataset_name, path=datasets_folder_path)\n",
    "    if demo_mode:\n",
    "        X_test, y_test = X_test[:2], y_test[:2]\n",
    "\n",
    "    for predictor_name in predictor_names:\n",
    "        print(\"Predictor: \", predictor_name)\n",
    "\n",
    "        # load classifier\n",
    "        classifier_filename = \"_\".join((predictor_name, dataset_name)) + \".pt\"\n",
    "        classifier = torch.load(os.path.join(trained_models_folder_path, predictor_name, classifier_filename),  map_location=torch.device(device)) # FileNotFoundError\n",
    "\n",
    "        # load explanations\n",
    "        attribution_filename = \"_\".join((\"all_results\", dataset_name, predictor_name)) + \".npy\"\n",
    "        explanations = np.load(os.path.join(attributions_folder_path, attribution_filename), allow_pickle=True).item() # FileNotFoundError\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "        for key in itertools.product(segmentation_names, background_names, normalization_names, metric_names):\n",
    "            segmentation_name, background_name, normalization_name, metric_name = key\n",
    "            print(\"assessing\", key)\n",
    "\n",
    "            # load model and explanations to access\n",
    "            try:\n",
    "                attributions = explanations['attributions'][dataset_name][segmentation_name][predictor_name][background_name][normalization_name]\n",
    "            except KeyError as error:\n",
    "                print('Warning: attributions is missing keys, skipping to next ' + repr(error))\n",
    "                continue\n",
    "\n",
    "            result = 1.0\n",
    "\n",
    "            data_list.append((dataset_name, segmentation_name, predictor_name, background_name, normalization_name, metric_name, None, result))\n",
    "\n",
    "            print(\"elapsed time\", (timeit.default_timer() -starttime))\n",
    "\n",
    "# res_file_name = \"demo_dict_result\" if demo_mode else \"dict_result\"\n",
    "# with open( \"_\".join( (res_file_name,dataset_name,classifier_name)) ,\"wb\") as f:\n",
    "#     pickle.dump(results_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ResNetBaseline' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[259], line 39\u001b[0m, in \u001b[0;36mAUC_difference\u001b[1;34m(X_train, X_test, y_train, dataset_name, segmentation_name, predictor_name, classifier, explanations, n_steps)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     opposite_class_dict[unique_class][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict_proba(opposite_sample[\u001b[38;5;28;01mNone\u001b[39;00m, :])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Nikos\\miniconda3\\envs\\env_segment_shap\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ResNetBaseline' object has no attribute 'predict_proba'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[259], line 83\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m AUC_diff\n\u001b[0;32m     81\u001b[0m n_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m18\u001b[39m\n\u001b[1;32m---> 83\u001b[0m AUC_difference(X_train, X_test, y_train, dataset_name, segmentation_name, predictor_name, classifier, explanations, n_steps\u001b[38;5;241m=\u001b[39mn_steps)\n",
      "Cell \u001b[1;32mIn[259], line 41\u001b[0m, in \u001b[0;36mAUC_difference\u001b[1;34m(X_train, X_test, y_train, dataset_name, segmentation_name, predictor_name, classifier, explanations, n_steps)\u001b[0m\n\u001b[0;32m     39\u001b[0m         opposite_class_dict[unique_class][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict_proba(opposite_sample[\u001b[38;5;28;01mNone\u001b[39;00m, :])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m         opposite_class_dict[unique_class][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(classifier\u001b[38;5;241m.\u001b[39mpredict(opposite_sample[\u001b[38;5;28;01mNone\u001b[39;00m, :])[\u001b[38;5;241m0\u001b[39m], ndmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample_idx, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X_test):\n\u001b[0;32m     44\u001b[0m     importance \u001b[38;5;241m=\u001b[39m normalize_saliency(attributions[sample_idx])\n",
      "File \u001b[1;32mc:\\Users\\Nikos\\miniconda3\\envs\\env_segment_shap\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ResNetBaseline' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "def normalize_saliency(saliency):\n",
    "    return (np.abs(saliency) + 1e-5) / (np.max(np.abs(saliency)) + 1e-5)\n",
    "\n",
    "def AUC_perturb_score(A, A_pred, B, B_pred, classifier):\n",
    "\n",
    "    preds = [A_pred]\n",
    "    perturbed_sample = A\n",
    "    for start_idx, end_idx in zip(start_points, end_points):\n",
    "        perturbed_idxs = salient_order[start_idx:end_idx]\n",
    "        perturbed_sample[perturbed_idxs] = B[perturbed_idxs]\n",
    "        perturbed_sample_reshaped = perturbed_sample.reshape(1, n_channels, n_timepoints)\n",
    "\n",
    "        try:\n",
    "            perturbed_sample_pred[unique_class][\"pred\"] = classifier.predict_proba(perturbed_sample_reshaped)[0][sample_class_pred_idx]\n",
    "        except AttributeError:\n",
    "            perturbed_sample_pred[unique_class][\"pred\"] = np.array(classifier.predict(perturbed_sample_reshaped)[0][sample_class_pred_idx], ndmin=1)\n",
    "\n",
    "        preds.append(perturbed_sample_pred)\n",
    "    preds.append(B_pred)\n",
    "    preds = (np.array(preds) - B_pred) * (A_pred - B_pred) # TODO  and fixed\n",
    "\n",
    "    AUC_score = sklearn_auc(x=np.arange(len(preds)), y=preds) / (len(preds))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def AUC_difference(X_train, X_test, y_train, dataset_name, segmentation_name, predictor_name, classifier, explanations, n_steps=18):\n",
    "\n",
    "    n_samples, n_channels, n_timepoints = X_test.shape\n",
    "    n_steps = np.min([n_steps, n_timepoints-2])\n",
    "\n",
    "    classes = np.unique(y_train) # TODO: load from label_mapping\n",
    "    opposite_class_dict = {}\n",
    "    for unique_class in classes:\n",
    "        opposite_class_idxs = ~(y_train==unique_class)\n",
    "        opposite_sample = np.mean(X_train[opposite_class_idxs], axis=0)\n",
    "        opposite_class_dict[unique_class] = {\"sample\": opposite_sample, \"pred\": None}\n",
    "        try:\n",
    "            opposite_class_dict[unique_class][\"pred\"] = classifier.predict_proba(opposite_sample[None, :])[0]\n",
    "        except AttributeError:\n",
    "            opposite_class_dict[unique_class][\"pred\"] = np.array(classifier.predict(opposite_sample[None, :])[0], ndmin=1)\n",
    "            # resnet direct and to numpy\n",
    "\n",
    "    AUC_diff_array = np.zeros(X_test.shape[0])\n",
    "\n",
    "    for sample_idx, sample in enumerate(X_test):\n",
    "        importance = normalize_saliency(attributions[sample_idx])\n",
    "        salient_order = np.argsort(importance.reshape(n_channels * n_timepoints))[::-1] # decreasing\n",
    "        change_points = np.array(np.round(np.linspace(0, n_channels * n_timepoints, n_steps + 2)), dtype=int)\n",
    "        start_points = change_points[:-2]\n",
    "        end_points = change_points[1:-1]\n",
    "\n",
    "        # deletion\n",
    "        try:\n",
    "            sample_pred = explanations['y_test_pred'][dataset_name][predictor_name][sample_idx]\n",
    "        except KeyError:\n",
    "            sample_pred = explanations['y_test_pred'][dataset_name][segmentation_name][predictor_name][sample_idx]\n",
    "        \n",
    "        sample_class_pred_idx = np.argmax(sample_pred)\n",
    "        sample_class = classes[sample_class_pred_idx]\n",
    "        sample_class_pred = sample_pred[sample_class_pred_idx]\n",
    "        opposite_sample = opposite_class_dict[sample_class][\"sample\"]\n",
    "        opposite_class_pred = opposite_class_dict[sample_class][\"pred\"][sample_class_pred_idx]\n",
    "\n",
    "        A = sample.copy()\n",
    "        A_pred = sample_class_pred\n",
    "        B = opposite_sample.copy()\n",
    "        B_pred = opposite_class_pred\n",
    "        AUC_deletion = AUC_perturb_score(A, A_pred, B, B_pred, classifier)\n",
    "\n",
    "        A = opposite_sample.copy()\n",
    "        A_pred = opposite_class_pred\n",
    "        B = sample.copy()\n",
    "        B_pred = sample_class_pred\n",
    "        AUC_insertion = AUC_perturb_score(A, A_pred, B, B_pred, classifier)\n",
    "\n",
    "        AUC_diff = AUC_insertion - AUC_deletion\n",
    "        AUC_diff_array[sample_idx] = AUC_diff\n",
    "\n",
    "    mean_AUC_diff = np.mean(AUC_diff_array)\n",
    "    return AUC_diff\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_steps = 18\n",
    "\n",
    "AUC_difference(X_train, X_test, y_train, dataset_name, segmentation_name, predictor_name, classifier, explanations, n_steps=n_steps)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Dataset', 'Segmentation', 'ML model', 'Background', 'Normalization', 'Perturb', 'Metric', \"Result\"]\n",
    "df = pd.DataFrame(data=data_list, columns = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Segmentation</th>\n",
       "      <th>ML model</th>\n",
       "      <th>Background</th>\n",
       "      <th>Normalization</th>\n",
       "      <th>Perturb</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gunpoint</td>\n",
       "      <td>clasp</td>\n",
       "      <td>resNet</td>\n",
       "      <td>zero</td>\n",
       "      <td>normalized</td>\n",
       "      <td>dummy</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gunpoint</td>\n",
       "      <td>clasp</td>\n",
       "      <td>resNet</td>\n",
       "      <td>zero</td>\n",
       "      <td>default</td>\n",
       "      <td>dummy</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dataset Segmentation ML model Background Normalization Perturb Metric  \\\n",
       "0  gunpoint        clasp   resNet       zero    normalized   dummy   None   \n",
       "1  gunpoint        clasp   resNet       zero       default   dummy   None   \n",
       "\n",
       "   Result  \n",
       "0     1.0  \n",
       "1     1.0  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"_\".join((\"evaluation\", predictor_name, dataset_name))\n",
    "result_path = os.path.join(cwd, \"results\", file_name)\n",
    "df.to_csv(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df.pivot(index=[\"Dataset\", \"Segmentation\", \"ML model\", \"Background\", \"Normalization\", \"Perturb\", \"Metric\"], columns=\"Result\", values=\"Result\")\n",
    "# df2.to_dict(orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_segment_shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
