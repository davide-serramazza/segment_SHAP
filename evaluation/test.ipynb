{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "#import torch.cuda\n",
    "import timeit\n",
    "import pandas as pd\n",
    "#import argparse\n",
    "import itertools\n",
    "from segment_SHAP.load_data import load_data # segment_SHAP\n",
    "from pickle import dump\n",
    "from utils import intantiate_dict_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "dataset_names = {'MP'}    #{sys.argv[1]}\n",
    "predictor_names = {'randomForest'}    #{sys.argv[2]} {\"randomForest\", 'miniRocket', 'resNet'}\n",
    "segmentation_names = {\"clasp\",\"greedygaussian\", \"equal\", \"infogain\",\"nnsegment\"} # {\"clasp\",\"greedygaussian\", \"equal\", \"infogain\",\"nnsegment\"} \n",
    "background_names =  {\"average\", \"zero\",\"sampling\"} #{\"average\", \"zero\", \"sampling\"}\n",
    "normalization_names = {\"default\", \"normalized\"}\n",
    "\n",
    "metric_names = {\"dummy\"}\n",
    "\n",
    "demo_mode = True\n",
    "# demo\n",
    "if demo_mode:\n",
    "    dataset_names = {'gunpoint'}\n",
    "    predictor_names = {'resNet'}\n",
    "    segmentation_names = {'clasp'}\n",
    "    background_names = {'zero'} #,'sampling'}\n",
    "    normalization_names = {\"default\", \"normalized\"}\n",
    "\n",
    "# optional\n",
    "# # get infos about which explanations are evaluated\n",
    "# datasets = list( explanations['attributions'].keys() )\n",
    "# segmentations = list( explanations['attributions'][datasets[0]].keys() )\n",
    "# predictors = list( explanations['attributions'][datasets[0]][segmentations[0]].keys() )\n",
    "# backgrounds = list( explanations['attributions'][datasets[0]][segmentations[0]][predictors[0]].keys() )\n",
    "# result_types = ['default','normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "datasets_folder_path = None\n",
    "attributions_folder_path = None\n",
    "trained_models_folder_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  gunpoint\n",
      "Predictor:  resNet\n",
      "assessing ('clasp', 'zero', 'normalized', 'dummy')\n",
      "elapsed time 0.05957539996597916\n",
      "assessing ('clasp', 'zero', 'default', 'dummy')\n",
      "elapsed time 0.05960319994483143\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "if datasets_folder_path is None:\n",
    "    datasets_folder_path = os.path.join(os.path.dirname(cwd), \"datasets\")\n",
    "if attributions_folder_path is None:\n",
    "    attributions_folder_path = os.path.join(os.path.dirname(cwd), \"attributions\")\n",
    "if trained_models_folder_path is None:\n",
    "    trained_models_folder_path = os.path.join(os.path.dirname(cwd), \"trained_models\")\n",
    "\n",
    "# device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "starttime = timeit.default_timer()\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(\"Dataset: \", dataset_name)\n",
    "    # loading dataset\n",
    "    X_train, X_test, y_train, y_test, enc = load_data(subset='all', dataset_name=dataset_name, path=datasets_folder_path)\n",
    "    if demo_mode:\n",
    "        X_test, y_test = X_test[:2], y_test[:2]\n",
    "\n",
    "    for predictor_name in predictor_names:\n",
    "        print(\"Predictor: \", predictor_name)\n",
    "\n",
    "        # load classifier\n",
    "        classifier_filename = \"_\".join((predictor_name, dataset_name)) + \".pt\"\n",
    "        classifier = torch.load(os.path.join(trained_models_folder_path, predictor_name, classifier_filename),  map_location=torch.device(device)) # FileNotFoundError\n",
    "\n",
    "        # load explanations\n",
    "        attribution_filename = \"_\".join((\"all_results\", dataset_name, predictor_name)) + \".npy\"\n",
    "        explanations = np.load(os.path.join(attributions_folder_path, attribution_filename), allow_pickle=True).item() # FileNotFoundError\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "        for key in itertools.product(segmentation_names, background_names, normalization_names, metric_names):\n",
    "            segmentation_name, background_name, normalization_name, metric_name = key\n",
    "            print(\"assessing\", key)\n",
    "\n",
    "            # load model and explanations to access\n",
    "            try:\n",
    "                attributions = explanations['attributions'][dataset_name][segmentation_name][predictor_name][background_name][normalization_name]\n",
    "            except KeyError as error:\n",
    "                print('Warning: attributions is missing keys, skipping to next ' + repr(error))\n",
    "                continue\n",
    "\n",
    "            result = 1.0\n",
    "\n",
    "            data_list.append((dataset_name, segmentation_name, predictor_name, background_name, normalization_name, metric_name, None, result))\n",
    "\n",
    "            print(\"elapsed time\", (timeit.default_timer() -starttime))\n",
    "\n",
    "\n",
    "\n",
    "# res_file_name = \"demo_dict_result\" if demo_mode else \"dict_result\"\n",
    "# with open( \"_\".join( (res_file_name,dataset_name,classifier_name)) ,\"wb\") as f:\n",
    "#     pickle.dump(results_dict,f)\n",
    "\n",
    "\n",
    "\n",
    "# file_name = \"_\".join((classifier_name, dataset_name))\n",
    "# result_path = os.path.join(cwd, \"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Dataset', 'Segmentation', 'ML model', 'Background', 'Normalization', 'Perturb', 'Metric', \"Result\"]\n",
    "df = pd.DataFrame(data=data_list, columns = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Segmentation</th>\n",
       "      <th>ML model</th>\n",
       "      <th>Background</th>\n",
       "      <th>Normalization</th>\n",
       "      <th>Perturb</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gunpoint</td>\n",
       "      <td>clasp</td>\n",
       "      <td>resNet</td>\n",
       "      <td>zero</td>\n",
       "      <td>normalized</td>\n",
       "      <td>dummy</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gunpoint</td>\n",
       "      <td>clasp</td>\n",
       "      <td>resNet</td>\n",
       "      <td>zero</td>\n",
       "      <td>default</td>\n",
       "      <td>dummy</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dataset Segmentation ML model Background Normalization Perturb Metric  \\\n",
       "0  gunpoint        clasp   resNet       zero    normalized   dummy   None   \n",
       "1  gunpoint        clasp   resNet       zero       default   dummy   None   \n",
       "\n",
       "   Result  \n",
       "0     1.0  \n",
       "1     1.0  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"_\".join((\"evaluation\", predictor_name, dataset_name))\n",
    "result_path = os.path.join(cwd, \"results\", file_name)\n",
    "df.to_csv(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df.pivot(index=[\"Dataset\", \"Segmentation\", \"ML model\", \"Background\", \"Normalization\", \"Perturb\", \"Metric\"], columns=\"Result\", values=\"Result\")\n",
    "# df2.to_dict(orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_segment_shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
